{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: How does data become the model?\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "    page-layout: full\n",
        "execute:\n",
        "  echo: false\n",
        "  warning: false\n",
        "fig-width: 10\n",
        "fig-height: 10\n",
        "---"
      ],
      "id": "ca10078a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "from matplotlib.patches import Patch\n",
        "import os \n",
        "from sklearn.metrics import mean_squared_error\n",
        "from verstack.stratified_continuous_split import scsplit\n",
        "\n",
        "rel_dir = '/home/kitadam/ENR_Sven/supervised_learning_jetpdb/data/jet-all-full.csv'\n",
        "jet_pdb_all = pd.read_csv(rel_dir)\n",
        "jet_pdb = jet_pdb_all[(jet_pdb_all['elongation'] != -1) & (jet_pdb_all['Zeff'] != -1)]\n",
        "jet_pdb['P_NBI(MW)'][jet_pdb['P_NBI(MW)'] < 0] = 0.0 \n",
        "jet_pdb['gasflowrateofmainspecies10^22(e/s)'][jet_pdb['gasflowrateofmainspecies10^22(e/s)'] < 0] = 0.0 \n",
        "jet_pdb.loc[:, 'divertorconfiguration'] = jet_pdb.loc[:, 'divertorconfiguration'].astype('category')\n",
        "jet_pdb.loc[:, 'divertorconfiguration'] = jet_pdb.loc[:, 'divertorconfiguration'].cat.codes\n",
        "jet_pdb['wall'] = [int(i) for i in (jet_pdb['shot'] < 80000).to_list()]\n",
        "global_cols = ['BetaN(MHD)', 'Zeff']\n",
        "info_cols = ['shot', 't1', 't2']\n",
        "ped_cols = ['nepedheight10^19(m^-3)', 'error_nepedheight10^19(m^-3)']\n",
        "mp_cols = ['Ip(MA)', 'B(T)', 'a(m)', 'q95','averagetriangularity', 'plasmavolume(m^3)','elongation','P_NBI(MW)', 'P_ICRH(MW)', 'P_TOT=PNBI+Pohm+PICRH-Pshi(MW)',   'gasflowrateofmainspecies10^22(e/s)', ]\n",
        "cat_cols = ['FLAG:Kicks', 'FLAG:RMP', 'FLAG:pellets', 'divertorconfiguration', 'Atomicnumberofseededimpurity']\n",
        "flags = ['FLAG:DEUTERIUM', 'FLAG:HYDROGEN', 'FLAG:H/Dmix', 'FLAG:HeJET-C', 'FLAG:Seeding', 'FLAG:Kicks', 'FLAG:RMP', 'FLAG:pellets', 'FLAG:HRTSdatavalidated', 'divertorconfiguration', 'Atomicnumberofseededimpurity',]\n",
        "if os.getenv('PLOTSTYLE') is not None: \n",
        "    plt.style.use(os.getenv('PLOTSTYLE'))\n",
        "RED = \"#dd3015\"\n",
        "GREEN = \"#489A8C\"\n",
        "DARK = \"#1C2C22\"\n",
        "GOLD = \"#F87D16\"\n",
        "WHITE = \"#FFFFFF\"\n",
        "BLUE = \"#2E6C96\""
      ],
      "id": "babba90c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Splitting of the dataset into subsets\n",
        "\n",
        "1. Shot number and or wall type\n",
        "    - Artificially create the idea of gathering more data via upgrades to the device or knowledge base \n"
      ],
      "id": "0537bf3f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig = plt.figure()\n",
        "plt.scatter(jet_pdb['shot'], 1e19*jet_pdb[ped_cols[0]], color=RED, edgecolors=(0, 0, 0))\n",
        "plt.axvline(81000, color=DARK, ls='--')\n",
        "plt.ylabel('JET PDB $n_e^{ped}$ (m$^{-3}$)')\n",
        "plt.xlabel('JET Shot Number')\n",
        "plt.annotate('JET-C', xy=(75500, 1.2e20))\n",
        "plt.annotate('JET-ILW', xy=(87500, 1.2e20))\n",
        "plt.show()"
      ],
      "id": "e5596407",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can preform regression on the dataset with the following splits: \n",
        "\n",
        "1. Both JET-C and JET-ILW\n",
        "2. JET-C \n",
        "3. JET-ILW \n",
        "\n",
        "And we can compare performances on all of the above by predicting against \n",
        "\n",
        "1. JET-ILW \n",
        "2. JET-C "
      ],
      "id": "e2e77863"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "jet_c = jet_pdb[jet_pdb['shot'] < 81000]\n",
        "jet_ilw = jet_pdb[jet_pdb['shot'] > 81000]\n",
        "lorenzo_inputs = ['Ip(MA)', 'averagetriangularity', 'P_TOT=PNBI+Pohm+PICRH-Pshi(MW)', 'gasflowrateofmainspecies10^22(e/s)', 'Meff']\n",
        "inputs = ['Ip(MA)', 'B(T)', 'a(m)', 'P_NBI(MW)', 'P_ICRH(MW)', 'P_TOT=PNBI+Pohm+PICRH-Pshi(MW)', 'plasmavolume(m^3)', 'q95', 'gasflowrateofmainspecies10^22(e/s)', 'elongation', 'averagetriangularity', 'FLAG:Kicks', 'FLAG:RMP', 'FLAG:pellets', 'Atomicnumberofseededimpurity', 'divertorconfiguration', 'wall']\n",
        "# inputs = ['Ip(MA)', 'B(T)', 'a(m)', 'P_TOT=PNBI+Pohm+PICRH-Pshi(MW)', 'plasmavolume(m^3)', 'q95', 'gasflowrateofmainspecies10^22(e/s)', 'elongation', 'averagetriangularity']\n",
        "\n",
        "targets = 'nepedheight10^19(m^-3)'\n",
        "\n",
        "# X_all, y_all = jet_pdb[lorenzo_inputs].to_numpy(), jet_pdb[targets].to_numpy()\n",
        "# X_c, y_c = jet_c[lorenzo_inputs].to_numpy(), jet_c[targets].to_numpy()\n",
        "# X_ilw, y_ilw = jet_ilw[lorenzo_inputs].to_numpy(), jet_ilw[targets].to_numpy()\n",
        "\n",
        "# X_all, y_all = jet_pdb[lorenzo_inputs], jet_pdb[targets]\n",
        "# X_c, y_c = jet_c[lorenzo_inputs], jet_c[targets]\n",
        "# X_ilw, y_ilw = jet_ilw[lorenzo_inputs], jet_ilw[targets]\n",
        "import random \n",
        "random.seed(42)\n",
        "shuffled_idxs = random.sample(jet_c.index.to_list(), k=len(jet_c))\n",
        "jet_c_train, jet_c_test = jet_c.loc[shuffled_idxs[:-50]], jet_c.loc[shuffled_idxs[-50:]]\n",
        "\n",
        "shuffled_idxs = random.sample(jet_ilw.index.to_list(), k=len(jet_ilw))\n",
        "jet_ilw_train, jet_ilw_test = jet_ilw.loc[shuffled_idxs[:-200]], jet_ilw.loc[shuffled_idxs[-200:]]"
      ],
      "id": "ba58a4c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a Figure, which doesn't have to be square.\n",
        "fig = plt.figure(layout='constrained', figsize=(10, 10), dpi=250)\n",
        "gs = fig.add_gridspec(2, 2,  width_ratios=(4, 1), height_ratios=(1, 4),\n",
        "                      left=0.1, right=0.9, bottom=0.1, top=0.9,\n",
        "                      wspace=0.05, hspace=0.05)\n",
        "ax = fig.add_subplot(gs[1, 0])\n",
        "\n",
        "# ax.scatter(jet_pdb['shot'], 1e19*jet_pdb[targets], color=RED, edgecolors=(0, 0, 0))\n",
        "ax.scatter(jet_c_train['shot'], 1e19*jet_c_train[targets], color=RED, edgecolors=(0, 0, 0))\n",
        "ax.scatter(jet_c_test['shot'], 1e19*jet_c_test[targets], color=WHITE, edgecolors=(0, 0, 0))\n",
        "ax.scatter(jet_ilw_train['shot'], 1e19*jet_ilw_train[targets], color=GREEN, edgecolors=(0, 0, 0))\n",
        "ax.scatter(jet_ilw_test['shot'], 1e19*jet_ilw_test[targets], color=GOLD, edgecolors=(0, 0, 0))\n",
        "\n",
        "ax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\n",
        "ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
        "ax_histy.hist([1e19*jet_c_train[targets], 1e19*jet_c_test[targets], 1e19*jet_ilw_train[targets], 1e19*jet_ilw_test[targets]], bins=50, orientation='horizontal', color=[RED, WHITE, GREEN, GOLD], stacked=True)\n",
        "# ax_histy.hist(1e19*jet_pdb[targets], bins=50, orientation='horizontal', color=RED)\n",
        "\n",
        "ax.axvline(81000, color=DARK, ls='--')\n",
        "ax.set_ylabel('JET PDB $n_e^{ped}$ (m$^{-3}$)')\n",
        "ax.set_xlabel('JET Shot Number')\n",
        "ax.annotate('JET-C', xy=(75500, 1.2e20))\n",
        "ax.annotate('JET-ILW', xy=(87500, 1.2e20))\n",
        "\n",
        "plt.show()"
      ],
      "id": "8901e5ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "import models\n",
        "import xgboost as xgb \n",
        "from pytorch_tabnet.tab_model import TabNetRegressor"
      ],
      "id": "a58bdf58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "while keeping the input parameters the same: \n",
        "\n",
        "- Lorenzo? \n"
      ],
      "id": "2274742b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train_c, y_train_c = jet_c_train[inputs], jet_c_train[targets]\n",
        "X_test_c, y_test_c = jet_c_test[inputs], jet_c_test[targets]\n",
        "\n",
        "X_train_ilw, y_train_ilw = jet_ilw_train[inputs], jet_ilw_train[targets]\n",
        "X_test_ilw, y_test_ilw = jet_ilw_test[inputs], jet_ilw_test[targets]\n",
        "\n",
        "reg_c_lin = LinearRegression().fit(X_train_c, y_train_c)\n",
        "c_lin_res_c = mean_squared_error(y_test_c, reg_c_lin.predict(X_test_c), squared=False)\n",
        "c_lin_res_ilw = mean_squared_error(y_test_ilw, reg_c_lin.predict(X_test_ilw), squared=False)\n",
        "\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train_c, label=y_train_c)\n",
        "dtest_c = xgb.DMatrix(X_test_c)\n",
        "dtest_ilw = xgb.DMatrix(X_test_ilw)\n",
        "# reg_c_tree = GradientBoostingRegressor().fit(X_train_c, y_train_c)\n",
        "params = dict(max_depth=11, learning_rate=0.1, objective='reg:squarederror')\n",
        "bst_c = xgb.train(params, dtrain, 100, [(dtrain, 'train')], verbose_eval=False)\n",
        "\n",
        "c_tree_res_c = mean_squared_error(y_test_c, bst_c.predict(dtest_c), squared=False)\n",
        "c_tree_res_ilw = mean_squared_error(y_test_ilw, bst_c.predict(dtest_ilw), squared=False)\n"
      ],
      "id": "cac5ea25",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train_c, y_train_c = jet_c_train[inputs], jet_c_train[targets]\n",
        "X_test_c, y_test_c = jet_c_test[inputs], jet_c_test[targets]\n",
        "\n",
        "X_train_ilw, y_train_ilw = jet_ilw_train[inputs], jet_ilw_train[targets]\n",
        "X_test_ilw, y_test_ilw = jet_ilw_test[inputs], jet_ilw_test[targets]\n",
        "\n",
        "X_train_both, y_train_both = pd.concat([X_train_ilw, X_train_c]), pd.concat([y_train_ilw, y_train_c])\n",
        "\n",
        "reg_cilw_lin = LinearRegression().fit(X_train_both, y_train_both)\n",
        "cilw_lin_res_c = mean_squared_error(y_test_c, reg_cilw_lin.predict(X_test_c), squared=False)\n",
        "cilw_lin_res_ilw = mean_squared_error(y_test_ilw, reg_cilw_lin.predict(X_test_ilw), squared=False)\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train_both, label=y_train_both)\n",
        "dtest_c = xgb.DMatrix(X_test_c)\n",
        "dtest_ilw = xgb.DMatrix(X_test_ilw)\n",
        "# reg_c_tree = GradientBoostingRegressor().fit(X_train_c, y_train_c)\n",
        "params = dict(max_depth=11, learning_rate=0.1, objective='reg:squarederror')\n",
        "bst_cilw = xgb.train(params, dtrain, 100, [(dtrain, 'train')], verbose_eval=False)\n",
        "\n",
        "cilw_tree_res_c = mean_squared_error(y_test_c, bst_cilw.predict(dtest_c), squared=False)\n",
        "cilw_tree_res_ilw = mean_squared_error(y_test_ilw, bst_cilw.predict(dtest_ilw), squared=False)"
      ],
      "id": "74949433",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "print('Train on JET-C')\n",
        "print('C    LIN: {:.4}   TREE: {:.4}'.format(c_lin_res_c, c_tree_res_c))\n",
        "print('ILW  LIN: {:.4}   TREE: {:.4}'.format(c_lin_res_ilw, c_tree_res_ilw))\n",
        "\n",
        "print('Train on JET-C & ILW')\n",
        "print('C    LIN: {:.4}   TREE: {:.4}'.format(cilw_lin_res_c, cilw_tree_res_c))\n",
        "print('ILW  LIN: {:.4}   TREE: {:.4}'.format(cilw_lin_res_ilw, cilw_tree_res_ilw))\n",
        "\n",
        "fig = plt.figure(figsize=(7.5, 7.5), dpi=100)\n",
        "\n",
        "plt.bar([-0.25, 0.25, 0.75, 1.25], [c_lin_res_c, cilw_lin_res_c, c_tree_res_c, cilw_tree_res_c], width=0.4, color=[RED, GREEN, RED, GREEN], edgecolor=(0, 0, 0))\n",
        "plt.xticks([0, 1], ['Lin. Reg.', 'XGBoost'], rotation=0)\n",
        "\n",
        "\n",
        "\n",
        "legend_elements = [Patch(facecolor=RED, edgecolor='black', label='JET-C'), Patch(facecolor=GREEN, edgecolor='black',label='JET-C & JET-ILW')]\n",
        "\n",
        "# Create the figure\n",
        "plt.legend(handles=legend_elements, title='Dataset used')\n",
        "plt.grid()\n",
        "plt.ylabel('RMSE on JET-C Subset')\n",
        "plt.show()"
      ],
      "id": "760689fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lin_results_c, lin_results_cilw, xgboost_results_c, xgboost_results_cilw = [], [], [], []\n",
        "lin_results_ilwc, xgboost_results_ilwc, lin_results_ilw, xgboost_results_ilw = [], [], [], []\n",
        "for seed in range(15):\n",
        "  random.seed(seed)\n",
        "  shuffled_idxs = random.sample(jet_c.index.to_list(), k=len(jet_c))\n",
        "  jet_c_train, jet_c_test = jet_c.loc[shuffled_idxs[:-50]], jet_c.loc[shuffled_idxs[-50:]]\n",
        "\n",
        "  shuffled_idxs = random.sample(jet_ilw.index.to_list(), k=len(jet_ilw))\n",
        "  jet_ilw_train, jet_ilw_test = jet_ilw.loc[shuffled_idxs[:-200]], jet_ilw.loc[shuffled_idxs[-200:]]\n",
        "\n",
        "  X_train_c, y_train_c = jet_c_train[inputs], jet_c_train[targets]\n",
        "  X_test_c, y_test_c = jet_c_test[inputs], jet_c_test[targets]\n",
        "\n",
        "  X_train_ilw, y_train_ilw = jet_ilw_train[inputs], jet_ilw_train[targets]\n",
        "  X_test_ilw, y_test_ilw = jet_ilw_test[inputs], jet_ilw_test[targets]\n",
        "\n",
        "  reg_c_lin = LinearRegression().fit(X_train_c, y_train_c)\n",
        "  c_lin_res_c = mean_squared_error(y_test_c, reg_c_lin.predict(X_test_c), squared=False)\n",
        "  c_lin_res_ilw = mean_squared_error(y_test_ilw, reg_c_lin.predict(X_test_ilw), squared=False)\n",
        "\n",
        "  dtrain = xgb.DMatrix(X_train_c, label=y_train_c)\n",
        "  dtest_c = xgb.DMatrix(X_test_c)\n",
        "  dtest_ilw = xgb.DMatrix(X_test_ilw)\n",
        "  params = dict(max_depth=11, learning_rate=0.1, objective='reg:squarederror')\n",
        "  bst_c = xgb.train(params, dtrain, 100, [(dtrain, 'train')], verbose_eval=False)\n",
        "\n",
        "  c_tree_res_c = mean_squared_error(y_test_c, bst_c.predict(dtest_c), squared=False)\n",
        "  c_tree_res_ilw = mean_squared_error(y_test_ilw, bst_c.predict(dtest_ilw), squared=False)\n",
        "  \n",
        "  lin_results_c.append(c_lin_res_c)\n",
        "  xgboost_results_c.append(c_tree_res_c)\n",
        "\n",
        "  lin_results_ilwc.append(c_lin_res_ilw)\n",
        "  xgboost_results_ilwc.append(c_tree_res_ilw)\n",
        "\n",
        "  X_train_both, y_train_both = pd.concat([X_train_ilw, X_train_c]), pd.concat([y_train_ilw, y_train_c])\n",
        "\n",
        "  reg_cilw_lin = LinearRegression().fit(X_train_both, y_train_both)\n",
        "  cilw_lin_res_c = mean_squared_error(y_test_c, reg_cilw_lin.predict(X_test_c), squared=False)\n",
        "  cilw_lin_res_ilw = mean_squared_error(y_test_ilw, reg_cilw_lin.predict(X_test_ilw), squared=False)\n",
        "\n",
        "  dtrain = xgb.DMatrix(X_train_both, label=y_train_both)\n",
        "  dtest_c = xgb.DMatrix(X_test_c)\n",
        "  dtest_ilw = xgb.DMatrix(X_test_ilw)\n",
        "  # reg_c_tree = GradientBoostingRegressor().fit(X_train_c, y_train_c)\n",
        "  params = dict(max_depth=11, learning_rate=0.1, objective='reg:squarederror')\n",
        "  bst_cilw = xgb.train(params, dtrain, 100, [(dtrain, 'train')], verbose_eval=False)\n",
        "\n",
        "  cilw_tree_res_c = mean_squared_error(y_test_c, bst_cilw.predict(dtest_c), squared=False)\n",
        "  cilw_tree_res_ilw = mean_squared_error(y_test_ilw, bst_cilw.predict(dtest_ilw), squared=False)\n",
        "\n",
        "  # tabnet_cilw = TabNetRegressor()\n",
        "  # tabnet_cilw.fit(X_train_both.values, y_train_both.values.reshape(-1, 1))\n",
        "  # cilw_tabnet_res_c = mean_squared_error(y_test_c.values, tabnet_cilw.predict(X_test_c.values), squared=False)\n",
        "  \n",
        "  # tabnet_results_cilw.append(cilw_tabnet_res_c)\n",
        "  lin_results_cilw.append(cilw_lin_res_c)\n",
        "  xgboost_results_cilw.append(cilw_tree_res_c)\n",
        "  lin_results_ilw.append(cilw_lin_res_ilw)\n",
        "  xgboost_results_ilw.append(cilw_tree_res_ilw)"
      ],
      "id": "86521eab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig = plt.figure(figsize=(10, 10), dpi=300)\n",
        "\n",
        "bbox_plot = plt.boxplot([lin_results_c, lin_results_cilw, xgboost_results_c, xgboost_results_cilw], vert=True, patch_artist=True, medianprops = dict(linestyle='-', linewidth=2.5, color=WHITE))\n",
        "\n",
        "colors = [RED, GREEN, RED, GREEN]\n",
        "for patch, color in zip(bbox_plot['boxes'], colors):\n",
        "  patch.set_facecolor(color)\n",
        "plt.xticks([1.5, 3.5], ['Lin. Reg.', 'XGBoost'])\n",
        "plt.ylabel('RMSE on JET-C Subset')\n",
        "legend_elements = [Patch(facecolor=RED, edgecolor='black', label='JET-C'), Patch(facecolor=GREEN, edgecolor='black',label='JET-C & JET-ILW')]\n",
        "\n",
        "# Create the figure\n",
        "plt.legend(handles=legend_elements, title='Dataset used')\n",
        "plt.show()"
      ],
      "id": "62830399",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "fig = plt.figure(figsize=(10, 10), dpi=300)\n",
        "\n",
        "bbox_plot = plt.boxplot([lin_results_ilwc, lin_results_ilw, xgboost_results_ilwc, xgboost_results_ilw], vert=True, patch_artist=True, medianprops = dict(linestyle='-', linewidth=2.5, color=WHITE))\n",
        "\n",
        "colors = [RED, GREEN, RED, GREEN]\n",
        "for patch, color in zip(bbox_plot['boxes'], colors):\n",
        "  patch.set_facecolor(color)\n",
        "plt.xticks([1.5, 3.5], ['Lin. Reg.', 'XGBoost'])\n",
        "plt.ylabel('RMSE on JET-ILW Subset')\n",
        "legend_elements = [Patch(facecolor=RED, edgecolor='black', label='JET-C'), Patch(facecolor=GREEN, edgecolor='black',label='JET-C & JET-ILW')]\n",
        "\n",
        "# Create the figure\n",
        "plt.legend(handles=legend_elements, title='Dataset used')\n",
        "plt.show()"
      ],
      "id": "58b927c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "seed = np.argmin(xgboost_results_c)\n",
        "shuffled_idxs = random.sample(jet_c.index.to_list(), k=len(jet_c))\n",
        "jet_c_train, jet_c_test = jet_c.loc[shuffled_idxs[:-50]], jet_c.loc[shuffled_idxs[-50:]]\n",
        "\n",
        "shuffled_idxs = random.sample(jet_ilw.index.to_list(), k=len(jet_ilw))\n",
        "jet_ilw_train, jet_ilw_test = jet_ilw.loc[shuffled_idxs[:-200]], jet_ilw.loc[shuffled_idxs[-200:]]\n",
        "\n",
        "X_train_c, y_train_c = jet_c_train[inputs], jet_c_train[targets]\n",
        "X_test_c, y_test_c = jet_c_test[inputs], jet_c_test[targets]\n",
        "\n",
        "X_train_ilw, y_train_ilw = jet_ilw_train[inputs], jet_ilw_train[targets]\n",
        "X_test_ilw, y_test_ilw = jet_ilw_test[inputs], jet_ilw_test[targets]\n",
        "\n",
        "X_train_both, y_train_both = pd.concat([X_train_ilw, X_train_c]), pd.concat([y_train_ilw, y_train_c])\n",
        "\n",
        "reg_c_lin = LinearRegression().fit(X_train_c, y_train_c)\n",
        "reg_cilw_lin = LinearRegression().fit(X_train_both, y_train_both)\n",
        "\n",
        "fig = plt.figure() \n",
        "\n",
        "plt.scatter(y_test_c, reg_c_lin.predict(X_test_c), color=RED)\n",
        "plt.scatter(y_test_c, reg_cilw_lin.predict(X_test_c), color=GREEN)\n",
        "lb, ub = 1, 12.5\n",
        "reg_x = np.linspace(lb, ub)\n",
        "plt.plot(reg_x, reg_x, lw=2, color='black')\n",
        "plt.plot(reg_x, reg_x*1.2, lw=2, color='black', ls='--')\n",
        "plt.plot(reg_x, reg_x*0.8, lw=2, color='black', ls='--')\n",
        "plt.xlim(lb, ub)\n",
        "plt.ylim(lb, ub)\n",
        "plt.xlabel('True $n_e^{ped}$ (m$^{-3}$)')\n",
        "plt.ylabel('Predicted $n_e^{ped}$ (m$^{-3}$)')\n",
        "plt.legend(frameon=False)\n",
        "\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train_c, label=y_train_c)\n",
        "dtest_c = xgb.DMatrix(X_test_c)\n",
        "params = dict(max_depth=11, learning_rate=0.1, objective='reg:squarederror')\n",
        "bst_c = xgb.train(params, dtrain, 100, [(dtrain, 'train')], verbose_eval=False)\n",
        "\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train_both, label=y_train_both)\n",
        "dtest_c = xgb.DMatrix(X_test_c)\n",
        "# reg_c_tree = GradientBoostingRegressor().fit(X_train_c, y_train_c)\n",
        "bst_cilw = xgb.train(params, dtrain, 100, [(dtrain, 'train')], verbose_eval=False)\n",
        "\n",
        "plt.scatter(y_test_c, bst_c.predict(dtest_c), color=RED, marker='*')\n",
        "plt.scatter(y_test_c, bst_cilw.predict(dtest_c), color=GREEN, marker='*')\n",
        "\n",
        "plt.show()"
      ],
      "id": "17233a9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "reg = LinearRegression().fit(X_c, y_c)\n",
        "fig = plt.figure() \n",
        "plt.title('Linear regressor fit on JET-C data')\n",
        "plt.scatter(y_c, reg.predict(X_c), color=RED, edgecolors=(0, 0, 0), label='JET-C: RMSE={:.4}'.format(mean_squared_error(y_c, reg.predict(X_c), squared=False)))\n",
        "plt.scatter(y_ilw, reg.predict(X_ilw), color=GREEN, edgecolors=(0, 0, 0), label='JET-ILW: RMSE={:.4}'.format(mean_squared_error(y_ilw, reg.predict(X_ilw), squared=False)))\n",
        "lb, ub = 1, 12.5\n",
        "reg_x = np.linspace(lb, ub)\n",
        "plt.plot(reg_x, reg_x, lw=2, color='black')\n",
        "plt.plot(reg_x, reg_x*1.2, lw=2, color='black', ls='--')\n",
        "plt.plot(reg_x, reg_x*0.8, lw=2, color='black', ls='--')\n",
        "plt.xlim(lb, ub)\n",
        "plt.ylim(lb, ub)\n",
        "plt.xlabel('True $n_e^{ped}$ (m$^{-3}$)')\n",
        "plt.ylabel('Predicted $n_e^{ped}$ (m$^{-3}$)')\n",
        "plt.legend(frameon=False)\n",
        "# reg.coef_\n",
        "# reg.intercept_\n",
        "# reg.predict(np.array([[3, 5]]))"
      ],
      "id": "be8b6615",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "reg_c = LinearRegression().fit(X_c, y_c)\n",
        "print('Regressor trained on JET-C')\n",
        "print('JET-ILW: RMSE = {:.4}'.format(mean_squared_error(y_ilw, reg_c.predict(X_ilw), squared=False)))\n",
        "print('JET-C: RMSE = {:.4}'.format(mean_squared_error(y_c, reg_c.predict(X_c), squared=False)))\n",
        "print()\n",
        "\n",
        "reg_ilw = LinearRegression().fit(X_ilw, y_ilw)\n",
        "print('Regressor trained on JET-ILW')\n",
        "print('JET-ILW: RMSE = {:.4}'.format(mean_squared_error(y_ilw, reg_ilw.predict(X_ilw), squared=False)))\n",
        "print('JET-C: RMSE = {:.4}'.format(mean_squared_error(y_c, reg_ilw.predict(X_c), squared=False)))\n",
        "print()\n",
        "\n",
        "reg_both = LinearRegression().fit(X_all, y_all)\n",
        "print('Regressor trained on JET-ILW and JET-C')\n",
        "print('JET-ILW: RMSE = {:.4}'.format(mean_squared_error(y_ilw, reg_both.predict(X_ilw), squared=False)))\n",
        "print('JET-C: RMSE = {:.4}'.format(mean_squared_error(y_c, reg_both.predict(X_c), squared=False)))\n",
        "print()\n"
      ],
      "id": "b02151e7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "def norm_mps(mps, mp_means, mp_stds): \n",
        "    return (mps - mp_means) / mp_stds\n",
        "\n",
        "\n",
        "def get_cv_iterator(X, y, num_cv: int = 15):\n",
        "    X, y = X.reset_index(drop=True), y.reset_index(drop=True)\n",
        "    train_size, test_size = 0.8, 0.2\n",
        "    X_trainval, X_test, y_trainval, y_test = scsplit(X, y, stratify=y, train_size=train_size, random_state=18)\n",
        "    val_size = 0.1\n",
        "    X_test_np, y_test_np =  X_test.to_numpy(), y_test.to_numpy()\n",
        "    X_trainval, y_trainval = X_trainval.reset_index(drop=True), y_trainval.reset_index(drop=True)\n",
        "    \n",
        "    for rng in list(range(num_cv)):\n",
        "        X_train, X_val, y_train, y_val = scsplit(X_trainval, y_trainval, stratify=y_trainval, test_size=val_size, random_state=rng)\n",
        "        train_mps_np, val_mps_np = X_train.to_numpy(), X_val.to_numpy(),\n",
        "        train_nepeds_np, val_nepeds_np = y_train.to_numpy(), y_val.to_numpy()\n",
        "        test_mps_np = X_test_np.copy()\n",
        "        if False: \n",
        "            # Can only normalize the numerical features in this case\n",
        "            num_numerical = X_test_np.shape[-1]\n",
        "            mp_means, mp_stds = np.mean(train_mps_np, axis=0), np.std(train_mps_np, axis=0)            \n",
        "            train_mps_np[:, :num_numerical], val_mps_np[:, :num_numerical], test_mps_np[:, :num_numerical] = norm_mps(train_mps_np[:, :num_numerical], mp_means[:num_numerical], mp_stds[:num_numerical]), norm_mps(val_mps_np[:, :num_numerical], mp_means[:num_numerical], mp_stds[:num_numerical]), norm_mps(test_mps_np[:, :num_numerical], mp_means[:num_numerical], mp_stds[:num_numerical])\n",
        "        else: \n",
        "            mp_means, mp_stds = np.mean(train_mps_np, axis=0), np.std(train_mps_np, axis=0)\n",
        "            train_mps_np, val_mps_np, test_mps_np = norm_mps(train_mps_np, mp_means, mp_stds), norm_mps(val_mps_np, mp_means, mp_stds), norm_mps(test_mps_np, mp_means, mp_stds)\n",
        "        yield (train_mps_np, train_nepeds_np), (val_mps_np, val_nepeds_np), (test_mps_np, y_test_np), (mp_means, mp_stds)\n",
        "\n",
        "X_all, y_all = jet_pdb[lorenzo_inputs], jet_pdb[targets]\n",
        "X_c, y_c = jet_c[lorenzo_inputs], jet_c[targets]\n",
        "X_ilw, y_ilw = jet_ilw[lorenzo_inputs], jet_ilw[targets]\n",
        "carbon_iterator = get_cv_iterator(X_c, y_c, 15)\n",
        "ilw_iterator = get_cv_iterator(X_ilw, y_ilw, 15)\n",
        "both_iterator = get_cv_iterator(X_all, y_all, 15)"
      ],
      "id": "5f9e2c49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "params = {'n_estimators': 500, 'learning_rate': 0.01}\n",
        "# carbon training \n",
        "test_score_c, test_score_ilw = 0.0, 0.0\n",
        "test_score_c_lin, test_score_ilw_lin = 0.0, 0.0\n",
        "for k, batch in enumerate(carbon_iterator): \n",
        "  train, valid, test, (mp_means, mp_stds) = batch \n",
        "  X_train, y_train = train\n",
        "  X_test, y_test = test\n",
        "  reg_c = GradientBoostingRegressor(**params).fit(X_train, y_train)\n",
        "  reg_c_lin = LinearRegression().fit(X_train, y_train)\n",
        "  test_score_ilw += mean_squared_error(y_ilw, reg_c.predict(norm_mps(X_ilw.to_numpy(), mp_means, mp_stds)), squared=False) \n",
        "  test_score_c += mean_squared_error(y_test, reg_c.predict(X_test), squared=False)\n",
        "  test_score_ilw_lin += mean_squared_error(y_ilw, reg_c_lin.predict(norm_mps(X_ilw.to_numpy(), mp_means, mp_stds)), squared=False) \n",
        "  test_score_c_lin += mean_squared_error(y_test, reg_c_lin.predict(X_test), squared=False)\n",
        "\n",
        "test_score_c /= (k+1)\n",
        "test_score_ilw /= (k+1)\n",
        "test_score_c_lin /= (k+1)\n",
        "test_score_ilw_lin /= (k+1)\n",
        "\n",
        "print('Regressor trained on JET-C')\n",
        "print('JET-ILW: Forest: {:.4}, Linear {:.4}'.format(test_score_ilw, test_score_ilw_lin), )\n",
        "print('JET-C: Forest = {:.4}, Linear {:.4}'.format(test_score_c, test_score_c_lin))\n",
        "print()"
      ],
      "id": "1554a6fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "# ILW training \n",
        "test_score_c, test_score_ilw = 0.0, 0.0\n",
        "for k, batch in enumerate(ilw_iterator): \n",
        "  train, valid, test, (mp_means, mp_stds) = batch \n",
        "  X_train, y_train = train\n",
        "  X_test, y_test = test\n",
        "  reg_ilw = GradientBoostingRegressor(**params).fit(X_train, y_train)\n",
        "  reg_ilw_lin = LinearRegression().fit(X_train, y_train)\n",
        "  test_score_c += mean_squared_error(y_c, reg_ilw.predict(norm_mps(X_c.to_numpy(), mp_means, mp_stds)), squared=False) \n",
        "  test_score_ilw += mean_squared_error(y_test, reg_ilw.predict(X_test), squared=False)\n",
        "  test_score_c_lin += mean_squared_error(y_c, reg_ilw_lin.predict(norm_mps(X_c.to_numpy(), mp_means, mp_stds)), squared=False) \n",
        "  test_score_ilw_lin += mean_squared_error(y_test, reg_ilw_lin.predict(X_test), squared=False)\n",
        "\n",
        "test_score_c /= (k+1)\n",
        "test_score_ilw /= (k+1)\n",
        "test_score_c_lin /= (k+1)\n",
        "test_score_ilw_lin /= (k+1)\n",
        "print('Regressor trained on JET-ILW')\n",
        "print('JET-ILW: Forest: {:.4}, Linear {:.4}'.format(test_score_ilw, test_score_ilw_lin), )\n",
        "print('JET-C: Forest = {:.4}, Linear {:.4}'.format(test_score_c, test_score_c_lin))\n",
        "print()"
      ],
      "id": "3c9cc14b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "test_score = 0.0\n",
        "test_score_lin = 0.0\n",
        "for k, batch in enumerate(both_iterator): \n",
        "  train, valid, test, (mp_means, mp_stds) = batch \n",
        "  X_train, y_train = train\n",
        "  X_test, y_test = test\n",
        "  reg_both = GradientBoostingRegressor(**params).fit(X_train, y_train)\n",
        "  reg_both_lin = LinearRegression().fit(X_train, y_train)\n",
        "  test_score += mean_squared_error(y_test, reg_both.predict(X_test), squared=False)\n",
        "  test_score_lin += mean_squared_error(y_test, reg_both_lin.predict(X_test), squared=False)\n",
        "\n",
        "test_score_lin /= k+1\n",
        "test_score /= k+1\n",
        "print('Regressor trained on JET-ILW and JET-C')\n",
        "print('Forest: {:.4}, Lin {:.4}'.format(test_score, test_score_lin))\n",
        "# print('JET-ILW: RMSE = {:.4}'.format(mean_squared_error(y_ilw, reg_both.predict(X_ilw), squared=False)))\n",
        "# print('JET-C: RMSE = {:.4}'.format(mean_squared_error(y_c, reg_both.predict(X_c), squared=False)))\n",
        "print()\n"
      ],
      "id": "9045659f",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}