---
title: "Some Automating"

execute: 
  echo: false
  warning: false

---


```{python}
import os 
import numpy as np
from helper_functions import get_allas_connection, get_dict_params
import matplotlib.pyplot as plt 
import matplotlib.gridspec as gridspec
conn = get_allas_connection()
# raw_dir = '/home/adam/ENR_Sven/moxie/data/raw/RAW_JET_PULSES'
raw_dir = '/home/kitadam/ENR_Sven/moxie/data/raw/RAW_JET_PULSES'

# rel_dir = '/home/kitadam/ENR_Sven/supervised_learning_jetpdb/data/jet-all-full.csv'
# jet_pdb_all = pd.read_csv(rel_dir)
shot_list = [fname for fname in os.listdir(raw_dir)]
import random 

random.seed(4)
if os.getenv('PLOTSTYLE') is not None: 
    plt.style.use(os.getenv('PLOTSTYLE'))

machine_param_signals = ['efit/Rgeo','efit/k', 'efit/ahor', 'efit/delRoben', 'efit/delRuntn', 'efit/Vol', 'gas/D_tot', 'magn/IpiFP', 'magn/q95',  'magn/BTF', 'power/PNBI_TOT', 'power/P_OH', 'power/PICR_TOT']
global_param_signals = ['bolo/topi', 'ehtr/wdia', 'kg1l/lad3'] 
elm_param_signals = ['edg8/tbeo', 'edg8/tbei']
diagnostic_signal_names = list(set(machine_param_signals + global_param_signals + elm_param_signals))

sample_shot_nums, sample_shot_dicts = [], []
for idx in random.sample(range(len(shot_list)), len(shot_list)): 
    if len(sample_shot_nums) == 9: 
        break 
    sample_shot = shot_list[idx]
    shot_dict = get_dict_params(sample_shot, diagnostic_signal_names, conn)
    if  shot_dict is not None: 
        sample_shot_dicts.append(shot_dict)
        sample_shot_nums.append(sample_shot)

```

Datasets in fusion are commonly but finding relevant time windows of desired signals.  

This process is commonly done by experts in the field, who know what they are looking for, but is a very manual instensive process. 
Take for example, the JET Pedestal Database @Frassinetti2021. 

- The time windows are within H-mode an manually selected
- The ELM timings are likely hand-checked then correlated with the profile measurements
- Fitting of profiles with MTANH

So we want to do this process, automatically and possibly compare models trained on JET PDB vs automated JET PDB. 

Ideally the end result is a [compiled executable](https://nuitka.net/). 
# What do we want to automate

We want H-mode electron density, temperature, and pressure measurements for pedestal analysis at/near the KBM-PB stability boundrary. To do so, we need to identify time windows of near constant or steady state plasma and machine parameters, then find identify when ELM's occured, and found profile measurements that land near the end of an ELM cycle $(75-99\%)$. 

1. Finding H-Mode 
2. Finding Steady-states inside H-mode
3. ELM timings 
4. Curve fitting

## 1. Finding H-mode

The amount of power needed to enter H-mode is commonly described as the L-H threshold, or $P_{LH}$. 

An example of a scaling law derived in terms of machine parameters is given in @Martin_2008: 

$$P_{\text{LH}} [\text{MW}] = 2.15e^{\pm 0.107} n_{e20}^{0.728 \pm 0.037} B_T^{0.772 \pm 0.031} a^{0.975\pm 0.08} R^{0.999\pm 0.101 }$$
where $B_T$ [T], $n_e$ [$10^{20}$ m $^{-2}$], $a$ [m], $R$ [m] are the magnitude of the toroidal magnetic field, line averaged electron density, major and minor plasma radius, respectively. The shaping parameters are sometimes refered to as $S$, or parameterized as the surface area. 
This can be intuitively thought of as given some volume of plasma, with surface area $S$ and density $n$, that is confined by a magnetic field $B$, there exists some amount of power needed to excited it to pass into a new regime. 

Thus, we are looking for the times where the total power injected is greator than $P_\text{LH}$:  $P_\text{TOT} \geq P_{\text{LH}}$, or $\frac{P_\text{TOT}}{P_{\text{LH}}} \geq 1.0$. 

For JET, the total power is typically coming from NBI, ICR, and Ohmic heating sources, $P_\text{NBI}, P_\text{ICR}, P_\text{OH}$ and there is some shine through lost deposited on the wall $P_\text{SH}$, thus $$P_\text{TOT} =  P_\text{NBI} + P_\text{ICR} + P_\text{OH} - P_\text{SH}$$
```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Code for finding P_TOT/P_LH"
from typing import Dict, List
from scipy.interpolate import interp1d 

def p_lh_threshold_martin_scaling_from_pulse_dict(pulse_dict: Dict[str, Dict[str, np.ndarray]]) -> List[np.ndarray]: 
    """ Using martin scaling we find the ratio of the PTOT/PLH 
    This is done by first interpolating all diagnostics used onto a common time domain 
    The common time domain is given by the lowest order sampled parameter used in the martin scaling 

    The power variables are then interpolated to the same axis 

    returns the time domain for the lh threshold, plh [MW], the ratio of ptot/plh
    """
    power_scaling = lambda n, r, a, b : (2.14)*(np.exp(0.107))*(np.power(n*1e-20,0.728))*(np.power(abs(b),0.772))*(np.power(a,0.975))*(np.power(r,0.999))
    lh_threshold_diagnostic_signal_names = ['kg1l/lad3', 'efit/Rgeo', 'efit/ahor', 'magn/BTF']
    # Finds the lowest sampled signal so we can interpolate onto that domain
    data_size_list = [(diag_sig, signal_dims['data'].shape) for diag_sig, signal_dims in pulse_dict.items() if diag_sig in lh_threshold_diagnostic_signal_names]
    domain_diagnostic, _ = min(data_size_list, key=lambda t: t[1]) # https://docs.python.org/3.8/library/functions.html#min
    diagnostics_to_interpolate = [d for d in lh_threshold_diagnostic_signal_names if d != domain_diagnostic]
    time_domain = pulse_dict[domain_diagnostic]['time'].copy()

    interpolated_dict = {domain_diagnostic: pulse_dict[domain_diagnostic]['data']}

    for key in diagnostics_to_interpolate: 
        _interp = interp1d(pulse_dict[key]['time'], pulse_dict[key]['data'])
        interp_data = _interp(time_domain)
        interpolated_dict[key] = interp_data

    plh_threshold = 1e6*power_scaling(*[interpolated_dict[key] for key in lh_threshold_diagnostic_signal_names])
    # Now to compare against the input power 
    power_diagnostic_signal_names = ['power/PNBI_TOT', 'power/P_OH', 'power/PICR_TOT', 'nbip/shi']
    total_power = np.zeros_like(time_domain)
    for key in power_diagnostic_signal_names: 
        try: 
            _interp = interp1d(pulse_dict[key]['time'], pulse_dict[key]['data'], bounds_error=False, fill_value=0.0)
            power_interped = _interp(time_domain)
        except ValueError: 
            power_interped = np.zeros_like(time_domain)
        except KeyError as e: 
            if e.args[0] == 'nbip/shi':
                power_interped = np.zeros_like(time_domain)
            else: 
                raise KeyError('pulse_dict does not contain the following necessary dignostics diagnostics: {}'.format(power_diagnostic_signal_names + lh_threshold_diagnostic_signal_names))
        if key in ['nbip/sh']: 
            total_power -= power_interped
        else:     
            total_power += power_interped
    ratio_threshold_total_power = total_power / plh_threshold
    # above_lh_threshold = time_domain[total_power > plh_threshold]
    # t_start, t_end = above_lh_threshold[0], above_lh_threshold[-1]
    return [time_domain, plh_threshold, ratio_threshold_total_power]


```

```{python}
#| eval: false
shot_number = 81768

example_dict = get_dict_params(shot_number, diagnostic_signal_names, conn)

time_domain, plh_threshold,ratio_threshold_total_power = p_lh_threshold_martin_scaling_from_pulse_dict(example_dict)
fig = plt.figure()
plt.title(f'JET #{shot_number}')
plt.plot(time_domain, ratio_threshold_total_power)
plt.xlabel('Time (s)')
plt.ylabel('$P_{TOT} / P_{LH}$')
plt.axhline(1.0, ls='--', color='black')
plt.show()


```

```{python}
#| label: lhratio-pulses
#| fig-cap: "This approach picks up the L-H transition, but not the back transition. If we were to use this to define the start and end of H-mode, it is prone to the following artifacts: i) When a PINI drops, the $P_{TOT} >= P_{LH}$ can drop below 1, even though we are still in H-mode, ii) near rampdown, any number of the parameters could spike, (see JET #86540 and 83304), iii) at the very beginning, there is a spike in density"

# sample_shot_nums = random.sample(shot_list, k=5)


fig1, f1_axes = plt.subplots(ncols=3, nrows=3, constrained_layout=True, figsize=(10, 10), sharex=True)
f1_axes = f1_axes.ravel()
for i, (shot_number, example_dict) in enumerate(zip(sample_shot_nums, sample_shot_dicts)): 
    if example_dict is None: 
        print(shot_number)
        continue 
    time_domain, plh_threshold,ratio_threshold_total_power = p_lh_threshold_martin_scaling_from_pulse_dict(example_dict)
    # f1_axes[i].subplot(len(sample_shot_nums), 1, k+1)
    f1_axes[i].set_title(f'JET #{shot_number}')
    f1_axes[i].plot(time_domain, ratio_threshold_total_power)
    _window = np.logical_and(ratio_threshold_total_power > 1.0, np.logical_and(time_domain > 44, time_domain < 60))
    f1_axes[i].plot(time_domain[_window], ratio_threshold_total_power[_window], color='red')
    f1_axes[i].scatter(time_domain[_window], ratio_threshold_total_power[_window], color='red', s=5)
    f1_axes[i].axvline(time_domain[_window][0], color='black', ls='--')
    f1_axes[i].axvline(time_domain[_window][-1], color='black', ls='--')
    f1_axes[i].set_xlabel('Time (s)')
    f1_axes[i].set_ylabel('$P_{TOT} / P_{LH}$ [-]')
    f1_axes[i].axhline(1.0, ls='--', color='black')
    f1_axes[i].set_xlim(40, 60)

    # f1_axes[i].yaxis.set_tick_params(which='both')

for ax in fig1.get_axes():
    ax.label_outer()
for ax in fig1.get_axes(): 
    ax.yaxis.set_tick_params(which='both', labelbottom=True)
    # plt.setp(ax.get_yticklabels(), visible=True)

fig1.suptitle('Ratio of martin scaling with total power')
plt.show()

```
#### But how to get the time windows for H-mode? 

## 2. Finding steady states
Here we want to find periods of near-constant global plasma parameters, namely the stored diamagnetic energy, $W_\text{DIA}$ [J]. The bounds for which a window or period of steady state can occur are made via: 

1. $\frac{P_{TOT}}{P_{LH}} >= 1$ 
2. times of the window $t$ exist between $44 < t < 60$ (for JET)
    - This is temporary, but should exclude times during rampdown and rampup where the LH-HL threshold can occur (although this should be investigated?)
    - Ideally, we pass a sliding window over the proposed $P_{ratio} > 1$ that removes outliers.  


#### Algorithm to determine where things are in the steady state 

1. Using [**rupture**](https://centre-borelli.github.io/ruptures-docs/)@Truong2020, find the change points in $W_\text{dia}$
    - The change points are separated by a minimum of 0.5 seconds 
2. Filter the change points selected to be those that fall between bounds set above 
3. The times bewteen change points, i.e., windows, are a steady state if the slope of $W_\text{DIA}$ between the change points is less than $15%$ the mean of the window. 
```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Code for finding steady states"

import ruptures as rpt 
from typing import Dict, List, Tuple
def find_steady_state_windows(pulse_dict: Dict[str, Dict[str, np.ndarray]]) -> Tuple[List[Tuple[float, float]], Tuple[float, float], Tuple[np.ndarray, np.ndarray]]: 
    """ Uses ruptures to find steady state windows using the PLH threshold and the diamagnetic energy """
    if not 'ehtr/wdia' in pulse_dict.keys(): raise KeyError('must include ehtr/wdia in list of diagnostics to pull')
    # FREE PARAMETERS 
    MIN_TIME_WINDOW_SIZE: float = 0.5
    MAX_DEVIATION_OF_MEAN_FROM_SLOP = 0.15 # Percent deviation for a window to be marked as a steady state
    lh_domain, _, ratio_threshold_total_power = p_lh_threshold_martin_scaling_from_pulse_dict(pulse_dict)
    h_mode_window = np.logical_and(ratio_threshold_total_power > 1.0, np.logical_and(lh_domain > 44, lh_domain < 60)) # TODO: This is not scalable for AUG
    t_start_hmode, t_end_hmode = lh_domain[h_mode_window][0], lh_domain[h_mode_window][-1]

    time, data = pulse_dict['ehtr/wdia']['time'], pulse_dict['ehtr/wdia']['data']
    dt = (time[-1] - time[0])/len(time)
    dt = np.diff(time).mean()
    window_size = int(MIN_TIME_WINDOW_SIZE / dt)
    if window_size < 100: 
        print(f'window_size too small {window_size}, the estimated sampling frequency is {dt:.5}')
        window_size = 1000 
    algo = rpt.Pelt(model='l2', min_size=window_size).fit(data)
    pen = np.log(len(time)) # This is tehcnically also a free parameter 
    my_bkps = algo.predict(pen=pen)
    t_bkps = np.array([time[bkp] for bkp in my_bkps if bkp != len(time)])
    t_bkps_in_hmode = t_bkps[np.logical_and(t_bkps > t_start_hmode, t_bkps <= t_end_hmode)]

    t_window_steady_states = []
    for t_idx, t in enumerate(t_bkps_in_hmode): 
        if t_idx == 0: 
            t_beg, t_end = t_start_hmode, t
        elif t_idx == len(t_bkps_in_hmode): 
            t_beg, t_end = t, t_end_hmode
        else: 
            t_beg, t_end = t_bkps_in_hmode[t_idx-1], t
        _window_bool = np.logical_and(time >= t_beg, time <= t_end)
        slope = (data[_window_bool][-1] - data[_window_bool][0]) / (time[_window_bool][-1] - time[_window_bool][0])
        if abs(slope / data[_window_bool].mean()) < MAX_DEVIATION_OF_MEAN_FROM_SLOP: 
            t_window_steady_states.append((t_beg, t_end))
    return t_window_steady_states, (t_start_hmode, t_end_hmode), (lh_domain, ratio_threshold_total_power)

```

```{python}
#| label: Steady-state
#| fig-cap: "This approach has various failure modes, namely i) uneven sampling rate of the WDIA signal leads to the minimum window size to be much less than 0.5, ii) the slope/mean calculation curently fails if the start and end are very similar values, so one should ideally include a std metric"

fig = plt.figure(figsize=(15, 15))
gs_init = gridspec.GridSpec(3, 3, figure=fig)

for k, (shot_number, example_dict) in enumerate(zip(sample_shot_nums, sample_shot_dicts)): 
    t_window_steady_states, ( t_start_hmode, t_end_hmode), (lh_domain, ratio_threshold_total_power) = find_steady_state_windows(example_dict)

    # col = 0 if k<= 1 else 1 
    if k<= 1: 
        col = 0 
    elif k <= 3: 
        col = 1 
    else: 
        col = 2
    row = k % 3

    gs_pulse = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=gs_init[k], hspace=0.0)
    ax1 = fig.add_subplot(gs_pulse[0])
    ax1.set_title(f'JET #{shot_number}')
    ax1.plot(lh_domain, ratio_threshold_total_power, label='$P_{TOT} / P_{LH}$')
    ax1.axhline(1.0, ls='--', color='grey')
    ax1.set_ylabel('-')

    ax2 = fig.add_subplot(gs_pulse[1])
    time, data = example_dict['ehtr/wdia']['time'], 1e-6*example_dict['ehtr/wdia']['data']
    ax2.plot(time, data, label='wdia')
    ax2.set_ylabel('MJ')

    for (t_beg, t_end) in t_window_steady_states: 
        _window_bool = np.logical_and(time >= t_beg, time <= t_end)
        ax2.plot(time[_window_bool], data[_window_bool], color='violet')
        ax2.axvline(t_beg, color='green', ls='--')
        ax2.axvline(t_end, color='green', ls='--')

    for ax in [ax1, ax2]: 
        ax.axvline(t_start_hmode, ls='--', color='black')
        ax.axvline(t_end_hmode, ls='--', color='black')
        ax.set_xlim(t_start_hmode - 1, t_end_hmode + 1)
    
for ax in fig.get_axes():
    # ax.set_xlim(40, 60)
    ax.legend(frameon=False)
    ax.grid()
    ax.label_outer()

plt.show()
```


## 3. ELM Timings 

For each time window $[t_1, t_2]$ given by the above steady state algorithm, we apply the following algorithm to determine $t_\text{ELM}$ (s): 

- Find the most likely ELM frequency, $\hat{f}_\text{ELM}$ of the window by taking the FFT of a normalised TBEO signal. The number of data points between ELMs is then likely to be $a\frac{\hat{f}_\text{ELM}}{T}$ where $T$ is the sampling frequency of the signal, and $a \in [0, 1]$ is a free parameter. Here I suggest using $a=0.66$, but offer no reasoning why.  


```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Code for finding ELM timings"
from scipy.fft import dct, idct, fft, fftfreq
from scipy import signal as sig
from typing import Dict, Tuple
def get_elm_timings_from_pulse_dict(pulse_dict: Dict[str, Dict[str, np.ndarray]], param='edg8/tbeo') -> Tuple[List[Tuple[float, float]], List[np.ndarray], Tuple[float, float]]: 
    t_window_steady_states, (t_start_hmode, t_end_hmode), (lh_domain, ratio_threshold_total_power) = find_steady_state_windows(pulse_dict)
    elm_param_signals = ['edg8/tbeo', 'edg8/tbei']

    elm_timings = []
    time, data = example_dict[param]['time'], example_dict[param]['data']
    for n, (t_beg, t_end) in enumerate(t_window_steady_states): 
        _window_bool = np.logical_and(time >= t_beg, time <= t_end)
        _time, _data = time[_window_bool], data[_window_bool]
        N = len(_time)
        T = np.diff(_time).mean() 
        _mean, _std = _data.mean(), _data.std()
        # To get rid of the close to 0 frequency in the fft, we subtract the mean and divide by std. 
        tbeo_dat_normed = (_data - _mean)/_std
        yf_og = fft(tbeo_dat_normed)
        xf = fftfreq(N, T)[:N//2]

        likely_freq_og = xf[np.argmax(2.0/N*np.abs(yf_og[0:N//2]))]
        likely_freq_time_og = 1.0/ likely_freq_og
        num_samples_patience_og = int(likely_freq_time_og / T)
        distance_in_indicie_space = num_samples_patience_og - (num_samples_patience_og // 3)
        peaks_og, properties_og = sig.find_peaks(_data,  height=_mean+_std, distance=distance_in_indicie_space) # TODO: Free parameter here! 
        window_elm_timings = _time[peaks_og]
        elm_timings.append(window_elm_timings)
    return t_window_steady_states, elm_timings, (t_start_hmode, t_end_hmode)

```
```{python}
#| label: ELM-timings
#| fig-cap: "Although this has many failure modes, it still gives a pretty good starting point to the ELM timings, which should in the end be done by hand (likely) or even clustering"
fig = plt.figure(figsize=(25, 20), constrained_layout=True)
gs_init = gridspec.GridSpec(len(sample_shot_nums), 1, figure=fig)
subfigs = [fig.add_subfigure(gs) for gs in gs_init]
for i, (shot_number, example_dict) in enumerate(zip(sample_shot_nums, sample_shot_dicts)): 
    t_window_steady_states, elm_timings_per_window, (t_start_hmode, t_end_hmode) = get_elm_timings_from_pulse_dict(example_dict)
    subfig = subfigs[i]
    subfig.suptitle(f'{shot_number} TBEO')
    axes = subfig.subplots(1, len(t_window_steady_states) + 1)
    ax1 = axes[0]
    time, data = example_dict['edg8/tbeo']['time'], example_dict['edg8/tbeo']['data']
    ax1.plot(time, data, label='tbeo')
    ax1.set_xlim(t_start_hmode -1, t_end_hmode+1)

    for n, ((t_beg, t_end), elm_timings) in enumerate(zip(t_window_steady_states, elm_timings_per_window)): 
        _window_bool = np.logical_and(time >= t_beg, time <= t_end)
        _time, _data = time[_window_bool], data[_window_bool]
        _mean, _std = _data.mean(), _data.std()
        ax1.plot(_time, _data, color='violet')
        ax1.axvline(t_beg, color='green', ls='--')
        ax1.axvline(t_end, color='green', ls='--')

        ax = axes[n+1]
        ax.plot(_time, _data, color='violet')
        ax.set_title(f'{t_end - t_beg:.2}s')
        ax.vlines(elm_timings, min(_data), _mean+_std, color='black', alpha=0.5)
        ax.set_yticks([], [])
        ax.set_xticks([t_beg, t_end], [f'{t_beg:.4}', f'{t_end:.4}'])
        ax.set_xlim(t_beg -0.1, t_end+0.1)
plt.show()
```


##### What does this look like in terms of the full pulse sequence for machine parameters? 


```{python}
#| label: pulse-sequence-with-time-sequences
#| fig-cap: "We see that the periods of steady state are often occuring with near constant machine parameters. One could likely study dynamics of the pedestal evolution, or just take an overall average for the whole pulse when MPS are the same."
from typing import Dict, Union, Optional
def plot_time_traces_with_time_windows(pulse_dict: Dict[str, Dict[str, np.ndarray]], shot_number: Union[int, str], time_windows: Optional[List[Tuple[float, float]]] = None, subfig = None) -> None: 
    machine_param_signals = ['efit/Rgeo','efit/k', 'efit/ahor', 'efit/delRoben', 'efit/delRuntn', 'efit/Vol', 'gas/D_tot', 'magn/IpiFP', 'magn/q95',  'magn/BTF', 'power/PNBI_TOT', 'power/P_OH', 'power/PICR_TOT']
    global_param_signals = ['bolo/topi', 'ehtr/wdia', 'kg1l/lad3', ]
    power_signal_names = ['power/PNBI_TOT', 'power/P_OH', 'power/PICR_TOT', 'nbip/shi']
    magnetic_signal_names = ['magn/IpiFP', 'magn/q95', 'magn/BTF']
    elm_param_signals = ['edg8/tbeo', 'edg8/tbei']
    group_dict = {'MA, -, T': [('magn/IpiFP', 1e-6), ('magn/q95', 1.0), ('magn/BTF', 1.0)],
                 'MW': [(sig, 1e-6) for sig in power_signal_names], 
                 '$10^{22}$ e/s': [('gas/D_tot', 1e-22)], 
                 '$10^{20}$ m$^{-3}$': [('kg1l/lad3', 1e-20)], 
                 'MJ, MW': [('bolo/topi', 1e-6), ('ehtr/wdia', 1e-6)], 
                 'ph/s/cm$^{2}$/sr': [('edg8/tbeo', 1e-13), ('edg8/tbei', 1e-13)]}

    if subfig is None: 
        subfig, fig_axs = plt.subplots(3, 2, figsize=(10, 10), sharex=True)
        plotting = True
    else:
        fig_axs = subfig.subplots(3, 2, sharex=True)
        plotting = False
    fig_axs = fig_axs.ravel()
    for i, (units, signal_scalar_tuple) in enumerate(group_dict.items()):
        for signal_name, scalar in signal_scalar_tuple: 
            if pulse_dict.get(signal_name) is None: 
                continue 
            time, data = pulse_dict[signal_name]['time'], abs(scalar*pulse_dict[signal_name]['data'])
            if isinstance(time, str):# time.shape != data.shape: 
                continue
            fig_axs[i].plot(time, data, label=signal_name.split('/')[-1], lw=5)
            if time_windows is not None: 
                for n, (t_beg, t_end) in enumerate(time_windows): 
                    _window_bool = np.logical_and(time >= t_beg, time <= t_end)
                    _time, _data = time[_window_bool], data[_window_bool]
                    _mean, _std = _data.mean(), _data.std()
                    fig_axs[i].scatter((_time[0] + _time[-1]) / 2.0, _mean, color='black', s=75, zorder=10)
                    
        fig_axs[i].set_ylabel(units)
        if units == 'MA, -, T': 
            fig_axs[i].set_ylim(0, 6)

    for ax in subfig.get_axes():
        ax.set_xlim(40, 60)
        ax.legend(frameon=False)
        ax.grid()

    subfig.subplots_adjust(hspace=0.0, wspace=0.4)
    subfig.suptitle(f'JET #{shot_number}')
    if plotting: 
        plt.show()

shot_number = sample_shot_nums[0]
# example_dict = get_dict_params(shot_number, diagnostic_signal_names, conn)
example_dict = sample_shot_dicts[0]
t_window_steady_states, _, _ = find_steady_state_windows(example_dict)
# print(example_dict.keys())
fig = plt.figure(figsize=(25, 25), constrained_layout=True)
gs_init = gridspec.GridSpec(3, 3, figure=fig)
subfigs = [fig.add_subfigure(gs) for gs in gs_init]

for i, (shot_number, example_dict) in enumerate(zip(sample_shot_nums, sample_shot_dicts)): 
    t_window_steady_states, _, _ = find_steady_state_windows(example_dict)
    plot_time_traces_with_time_windows(example_dict, shot_number, t_window_steady_states, subfig=subfigs[i])
plt.show()
```

- Machine parameters 
    - **Shaping**: $R$, $a$, $\delta_u$, $\delta_o$, $V_P$,  **Gas**: $\Gamma_D$, **Power**: Defined above, **Magnetic**: $q_{95}$, $I_P$, $B_T$
- Global Parameters
    - Radiated power from bolometer,  Diamagnetic Stored energy of the plasma, Line averaged density, Average inner/outer Be II photon flux (ELMs)

#### Notable other ELM algorithms 

- As implemented in @ELM_TIMINGS_PREPRINT 


## 4. Bayesian fitting 

this isn't really automating things but rather a better way to fit. 

```{python}

```


### Libraries used 

- Ruptures 
- Scipy 
- numpy 