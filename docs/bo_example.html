<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Musings - 2&nbsp; Bayesian Optimization Example for Model Validation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./bootstrap_current.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./bo_example.html">Bayes</a></li><li class="breadcrumb-item"><a href="./bo_example.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayesian Optimization Example for Model Validation</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Musings</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/fusionby2030/musings" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Tokamak</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bootstrap_current.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">On the bootstrap current in tokamaks</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bo_example.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayesian Optimization Example for Model Validation</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#example-model" id="toc-example-model" class="nav-link active" data-scroll-target="#example-model"><span class="header-section-number">2.1</span> Example Model</a></li>
  <li><a href="#framing-model-validation-as-an-optimization-problem" id="toc-framing-model-validation-as-an-optimization-problem" class="nav-link" data-scroll-target="#framing-model-validation-as-an-optimization-problem"><span class="header-section-number">2.2</span> Framing model validation as an optimization problem</a></li>
  <li><a href="#surrogate-model-of-the-discrepency-between-model-and-experiment-r" id="toc-surrogate-model-of-the-discrepency-between-model-and-experiment-r" class="nav-link" data-scroll-target="#surrogate-model-of-the-discrepency-between-model-and-experiment-r"><span class="header-section-number">2.3</span> Surrogate model of the discrepency between model and experiment (<span class="math inline">\(r\)</span>)</a>
  <ul class="collapse">
  <li><a href="#approach-1-point-wise-estimation-via-maximizing-the-log-likelihood" id="toc-approach-1-point-wise-estimation-via-maximizing-the-log-likelihood" class="nav-link" data-scroll-target="#approach-1-point-wise-estimation-via-maximizing-the-log-likelihood">Approach 1: Point-wise estimation via maximizing the log-likelihood</a></li>
  <li><a href="#approach-2-marginalizing-the-hyperparameters-out" id="toc-approach-2-marginalizing-the-hyperparameters-out" class="nav-link" data-scroll-target="#approach-2-marginalizing-the-hyperparameters-out">Approach 2: Marginalizing the hyperparameters out</a></li>
  </ul></li>
  <li><a href="#aquiring-new-points-from-the-surrogate" id="toc-aquiring-new-points-from-the-surrogate" class="nav-link" data-scroll-target="#aquiring-new-points-from-the-surrogate"><span class="header-section-number">2.4</span> Aquiring new points from the surrogate</a>
  <ul class="collapse">
  <li><a href="#approach-1-using-the-mle-surrogate" id="toc-approach-1-using-the-mle-surrogate" class="nav-link" data-scroll-target="#approach-1-using-the-mle-surrogate">Approach 1: Using the MLE surrogate</a></li>
  <li><a href="#approach-2-using-the-integrated-predictive-posterior" id="toc-approach-2-using-the-integrated-predictive-posterior" class="nav-link" data-scroll-target="#approach-2-using-the-integrated-predictive-posterior">Approach 2: Using the integrated predictive posterior</a></li>
  </ul></li>
  <li><a href="#the-full-bo-algorithm" id="toc-the-full-bo-algorithm" class="nav-link" data-scroll-target="#the-full-bo-algorithm"><span class="header-section-number">2.5</span> The full BO algorithm</a></li>
  <li><a href="#obtaining-uncertainties" id="toc-obtaining-uncertainties" class="nav-link" data-scroll-target="#obtaining-uncertainties"><span class="header-section-number">2.6</span> Obtaining uncertainties</a></li>
  <li><a href="#scaling-to-multiple-experiments" id="toc-scaling-to-multiple-experiments" class="nav-link" data-scroll-target="#scaling-to-multiple-experiments"><span class="header-section-number">2.7</span> Scaling to multiple experiments</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">2.8</span> References</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/fusionby2030/musings/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayesian Optimization Example for Model Validation</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Given some model <span class="math inline">\(f(x, \theta)&nbsp;\rightarrow y\)</span>, which takes inputs <span class="math inline">\(x\)</span> and free parameters <span class="math inline">\(\theta\)</span>, and maps them to <span class="math inline">\(y \in \mathbb{R}^d\)</span>, the validation exercise is the following:</p>
<p>given an observed measurement or pheonemna <span class="math inline">\(\hat{y}\)</span>, we aim to find <span class="math inline">\(\theta\)</span> such that <span class="math inline">\(f(x, \theta) \sim \hat{y}\)</span>. Once we find an optimal <span class="math inline">\(\theta\)</span>, our secondary goal is to assess i) how certain we are that <span class="math inline">\(\theta\)</span> is the optimal, and ii) how do changes in <span class="math inline">\(\theta\)</span> propagate through our model <span class="citation" data-cites="Wu2018">(<a href="#ref-Wu2018" role="doc-biblioref">Wu et al. 2018</a>)</span>.</p>
<section id="example-model" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="example-model"><span class="header-section-number">2.1</span> Example Model</h2>
<p>Let’s consider the model <span class="math display">\[f(x, \alpha, \beta) = \alpha\sin(x) + \beta e^{-x}\]</span></p>
<p>The free parameters are <span class="math inline">\(\alpha, \beta\)</span>.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>model_f <span class="op">=</span> <span class="kw">lambda</span> x, alpha, beta: (alpha<span class="op">*</span>np.sin(x) <span class="op">+</span> beta<span class="op">*</span>np.exp(<span class="op">-</span>x)) <span class="co">#  / gamma*np.cos(x)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>x_range <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, num_samples)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>): </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  alpha <span class="op">=</span> <span class="fl">4.0</span> <span class="op">*</span> np.random.randn()</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  beta <span class="op">=</span> <span class="fl">1.0</span> <span class="op">*</span> np.random.randn() <span class="op">+</span> <span class="dv">2</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  gamma <span class="op">=</span> <span class="dv">1</span> <span class="op">*</span> np.random.randn()</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  plt.plot(x_range, model_f(x_range, alpha, beta), label<span class="op">=</span><span class="vs">r'$\alpha$=</span><span class="sc">{alpha:.2}</span><span class="vs">; $\beta$=</span><span class="sc">{beta:.2}</span><span class="vs">'</span>.<span class="bu">format</span>(alpha<span class="op">=</span>alpha, beta<span class="op">=</span>beta))</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="vs">r'Example Model output for various $\theta$'</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r'$f(x, \alpha, \beta)$'</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="bo_example_files/figure-html/cell-2-output-1.png" width="610" height="450"></p>
</div>
</div>
<p>Let’s say we have some noisy measurements now that we want to validate the model against:</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>x_range <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.0</span>, <span class="fl">1.0</span>, num_samples)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>alpha_target, beta_target <span class="op">=</span> <span class="fl">7.9</span>, <span class="fl">1.4</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>experimental_result <span class="op">=</span> model_f(x_range, alpha_target, beta_target) <span class="op">+</span> <span class="fl">0.4</span><span class="op">*</span>np.random.randn(x_range.shape[<span class="dv">0</span>])</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>target_model <span class="op">=</span> model_f(x_range, alpha_target, beta_target)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure() </span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>plt.plot(x_range, target_model, label<span class="op">=</span><span class="vs">r'Target $y$; $\alpha$=</span><span class="sc">{alpha:.2}</span><span class="vs">; $\beta$=</span><span class="sc">{beta:.2}</span><span class="vs">'</span>.<span class="bu">format</span>(alpha<span class="op">=</span>alpha_target, beta<span class="op">=</span>beta_target), color<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>plt.scatter(x_range, experimental_result, label<span class="op">=</span><span class="st">'Experiment: $\hat</span><span class="sc">{y}</span><span class="st">$'</span>,)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="bo_example_files/figure-html/cell-3-output-1.png" width="590" height="434"></p>
</div>
</div>
<p>The goal is then to find <span class="math inline">\(\alpha = 7.9\)</span> and <span class="math inline">\(\beta = 1.4\)</span>. Additionally, it would be nice to know how certain we are about <span class="math inline">\(\alpha, \beta\)</span>.</p>
</section>
<section id="framing-model-validation-as-an-optimization-problem" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="framing-model-validation-as-an-optimization-problem"><span class="header-section-number">2.2</span> Framing model validation as an optimization problem</h2>
<p>Formally, we want to find <span class="math display">\[\underset{{\theta \in \mathcal{D}}}{\text{argmin}} ( r(f(x, \theta), \hat{y}))\]</span> where <span class="math inline">\(r(f, \hat{y})\)</span> is a mapping <span class="math inline">\(r: \mathbb{R}^d\times\mathbb{R}^d \rightarrow \mathbb{R}^+\)</span> describing the discrepency between the model output and experimental observation, and <span class="math inline">\(\mathcal{D}\)</span> is the space in which parameters <span class="math inline">\(\theta\)</span> are constrained.</p>
<p>For our example, we can use the L-2 norm as a discrepency function: <span class="math display">\[r(f(x, \theta), \hat{y}) = | f(x, \theta) - \hat{y} | ^2_2\]</span></p>
<p>To visualize the discrepency function, we can sample <span class="math inline">\(\alpha, \beta\)</span> and compute <span class="math inline">\(r\)</span>.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> colors <span class="im">as</span> col <span class="co"># .colors import Normalize</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">5</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>betas <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">5</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>disc_func <span class="op">=</span> <span class="kw">lambda</span> y, yhat: ((y <span class="op">-</span> yhat)<span class="op">**</span><span class="dv">2</span>).mean(<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> yhat.shape[<span class="dv">0</span>]<span class="co"># np.linalg.norm# lambda y, yhat: (y - yhat)**2</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> np.dstack(np.meshgrid(alphas, betas)).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>discs <span class="op">=</span> np.empty(grid.shape[<span class="dv">0</span>])</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, pair <span class="kw">in</span> <span class="bu">enumerate</span>(grid): </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  model_out <span class="op">=</span> model_f(x_range, <span class="op">*</span>pair)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>  discrepency <span class="op">=</span> disc_func(model_out, experimental_result)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>  discs[i] <span class="op">=</span> discrepency</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> plt.get_cmap(<span class="st">'plasma'</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>scmap <span class="op">=</span> plt.cm.ScalarMappable(col.Normalize(vmin<span class="op">=</span><span class="fl">0.0</span>, vmax<span class="op">=</span><span class="bu">max</span>(discs)), cm)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> scmap.to_rgba(discs)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> c, pair <span class="kw">in</span> <span class="bu">zip</span>(colors, grid): </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>  axs[<span class="dv">0</span>].plot(x_range, model_f(x_range, <span class="op">*</span>pair), c<span class="op">=</span>c)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].scatter(x_range, experimental_result, label<span class="op">=</span><span class="st">'Experiment: $\hat</span><span class="sc">{y}</span><span class="st">$'</span>, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].scatter(grid[:, <span class="dv">0</span>], grid[:, <span class="dv">1</span>], c<span class="op">=</span>discs, vmin<span class="op">=</span><span class="dv">0</span>, vmax<span class="op">=</span><span class="bu">max</span>(discs), cmap<span class="op">=</span>cm)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].scatter(alpha_target, beta_target, marker<span class="op">=</span><span class="st">'*'</span>, s<span class="op">=</span><span class="dv">200</span>, color<span class="op">=</span><span class="st">'Green'</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_xlabel(<span class="vs">r'$\alpha$'</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_ylabel(<span class="vs">r'$\beta$'</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_title(<span class="st">'Model outputs'</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_title(<span class="vs">r'Discrepency Function against various $\theta$'</span>)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>fig.colorbar(scmap, ax <span class="op">=</span> axs[<span class="dv">1</span>], label<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>fig.subplots_adjust(wspace<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="bo_example_files/figure-html/cell-4-output-1.png" width="672" height="377" class="figure-img"></p>
<figcaption class="figure-caption">Model outputs from draws of <span class="math inline">\(\alpha, \beta\)</span> compared to the experiment we want to model. The draws are coloured by their discrepency with the experiment (colorbar on the right plot), The draws of <span class="math inline">\(\alpha\)</span> (x-axis), <span class="math inline">\(\beta\)</span> (y-axis) coloured by the discrepency value (L2 norm) of the model output with those values.</figcaption>
</figure>
</div>
</div>
</div>
<p>In real world examples, we do not have direct (analytical) access to <span class="math inline">\(r\)</span>. Therefore, we want to make an approximate of <span class="math inline">\(r\)</span>.</p>
</section>
<section id="surrogate-model-of-the-discrepency-between-model-and-experiment-r" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="surrogate-model-of-the-discrepency-between-model-and-experiment-r"><span class="header-section-number">2.3</span> Surrogate model of the discrepency between model and experiment (<span class="math inline">\(r\)</span>)</h2>
<p><span class="math inline">\(r\)</span> can be approximated by any number of functions. Therefore, a Gaussian Process Regression (GPR) is a good start. A good overview of GPRs can be found in <span class="citation" data-cites="RasmussenW06">Rasmussen and Williams (<a href="#ref-RasmussenW06" role="doc-biblioref">2006</a>)</span> and <span class="citation" data-cites="Rasmussen1995">Williams and Rasmussen (<a href="#ref-Rasmussen1995" role="doc-biblioref">1995</a>)</span>. The points relevant to this discusison are that: i) GPR defines a family (distribution) of functions, normally Guassian, and ii) like any model, a GPR has hyperparameters, typically <span class="math inline">\(D + 3\)</span> parameters, for <span class="math inline">\(D\)</span> dimensionality of the data you want to fit (in this case 2). The traditional GPR thus learns a model that outputs a distribution: <span class="math display">\[\hat{r} \sim \mathcal{N}(\mu (\theta, \text{hyper}), \sigma (\theta, \text{hyper}))\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are deterimed by what are called the kernel and mean function of the GPR. Additionally, the kernel and mean function have hyperparameters, <span class="math inline">\(\text{hyper}\)</span> that we must fit. Another key point is that the kernel and mean function are differentiable functions.</p>
<p>We are only as good as our surrogate model. To find a good surrogate model, we must find the proper hyperparameters of the kernel and mean function of the GPR.</p>
<section id="approach-1-point-wise-estimation-via-maximizing-the-log-likelihood" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="approach-1-point-wise-estimation-via-maximizing-the-log-likelihood">Approach 1: Point-wise estimation via maximizing the log-likelihood</h3>
<p>In this example, we will use a Gaussian likelihood GPR, i.e., we say that the probability density of observing a point <span class="math inline">\(R = r(\theta)\)</span> that is generated by a Gaussian distribution is given by:</p>
<p><span class="math display">\[P(R, \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} \exp \left( -\frac{(R - \mu)^2}{2\sigma^2}\right)\]</span> where, one again, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are outputs of our GPR. Notice this probability distribution has a maximum if the mean output of our GPR matches that of the point we observe, i.e., <span class="math inline">\(\mu = R\)</span>. If we have mutliple points to fit, <span class="math inline">\(\vec{R} = (r(\theta_1), r(\theta_2), r(\theta_3), \dots, R_i)\)</span>, then the total joint probability distribution of observing all the points is given by the product of their likelihood:</p>
<p><span class="math display">\[P(\vec{R}, \mu, \sigma) = \prod_i \frac{1}{\sigma\sqrt{2\pi}} \exp \left( -\frac{(R_i - \mu)^2}{2\sigma^2}\right)\]</span></p>
<p>However, it is easy to see that with many points, we will likely hit some numerical underflow, therefore we can make use of the logarithm:, <span class="math display">\[\ln(P(\vec{R}, \mu, \sigma)) = i\ln \left(\frac{1}{\sigma \sqrt{2\pi}}\right) - \sum_i \left(\frac{(R_i - \mu)^2}{2\sigma^2}\right)\]</span></p>
<p>We can then differentiate this function in order to find the maximum and apply our favourite gradient based optimizer to find the hyperparameters of the kernel and mean function that determine <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. This approach is called Maximum Likelihood Estimation (MLE). <strong>Note</strong>: This is called point-wise because we are estimating the hyperparameters of the GP ‘per point’ we use to fit the GP.</p>
<p>Below is an example of the ‘lengthscale’ parameters of the covariance function, where the countours are the(negative) log-likelihood estimation.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gpytorch </span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> botorch </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>model_f_torch <span class="op">=</span> <span class="kw">lambda</span> x, alpha, beta: (alpha<span class="op">*</span>torch.sin(x) <span class="op">+</span> beta<span class="op">*</span>torch.exp(<span class="op">-</span>x)) </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>experimental_result_torch <span class="op">=</span> torch.from_numpy(experimental_result)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>disc_fun_torch <span class="op">=</span> torch.nn.MSELoss(reduction<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>train_x <span class="op">=</span> torch.from_numpy(grid).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>x_range_train <span class="op">=</span> torch.tile(torch.from_numpy(x_range), (train_x.shape[<span class="dv">0</span>], <span class="dv">1</span>))</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>model_output <span class="op">=</span> model_f_torch(x_range_train, train_x[:, <span class="dv">0</span>].unsqueeze(<span class="op">-</span><span class="dv">1</span>), train_x[:, <span class="dv">1</span>].unsqueeze(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>train_y <span class="op">=</span> disc_fun_torch(experimental_result_torch.repeat((model_output.shape[<span class="dv">0</span>], <span class="dv">1</span>)), model_output).mean(<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> x_range_train.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># We will use the simplest form of GP model, exact inference</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ExactGPModel(gpytorch.models.ExactGP, botorch.models.gpytorch.GPyTorchModel):</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    num_outputs <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, train_x, train_y, likelihood):</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ExactGPModel, <span class="va">self</span>).<span class="fu">__init__</span>(train_x, train_y, likelihood)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mean_module <span class="op">=</span> gpytorch.means.ConstantMean(constant_prior<span class="op">=</span>gpytorch.priors.NormalPrior(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.covar_module <span class="op">=</span> gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims<span class="op">=</span>train_x.shape[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        mean_x <span class="op">=</span> <span class="va">self</span>.mean_module(x)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        covar_x <span class="op">=</span> <span class="va">self</span>.covar_module(x)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> gpytorch.distributions.MultivariateNormal(mean_x, covar_x)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize likelihood and model</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>likelihood <span class="op">=</span> gpytorch.likelihoods.GaussianLikelihood()</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ExactGPModel(train_x, train_y, likelihood)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>mll <span class="op">=</span> gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>model.likelihood.noise <span class="op">=</span> torch.tensor(<span class="fl">0.1</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>model.mean_module.constant <span class="op">=</span> torch.tensor(<span class="fl">5.750</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>model.covar_module.outputscale <span class="op">=</span> torch.tensor(<span class="fl">5.786</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>model.covar_module.base_kernel.lengthscale <span class="op">=</span> torch.tensor([<span class="fl">5.3</span>, <span class="fl">3.535</span>])</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>lscale_1_range <span class="op">=</span> torch.linspace(<span class="fl">0.1</span>, <span class="dv">20</span>, <span class="dv">100</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>lscale_2_range <span class="op">=</span> torch.linspace(<span class="fl">0.1</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> np.empty((<span class="dv">100</span>, <span class="dv">100</span>))</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, lscale_1 <span class="kw">in</span> <span class="bu">enumerate</span>(lscale_1_range): </span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> j, lscale_2 <span class="kw">in</span> <span class="bu">enumerate</span>(lscale_2_range): </span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    model.covar_module.base_kernel.lengthscale <span class="op">=</span> torch.tensor([lscale_1,lscale_2])</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># model.likelihood.noise = torch.tensor(noiscale.item())</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(train_x)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="op">-</span>mll(output, train_y)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    image[i, j] <span class="op">=</span> loss</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure() </span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>cax <span class="op">=</span> plt.contour(lscale_1_range, lscale_2_range, image, <span class="dv">50</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r'Length scale $\alpha$'</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r'Length scale $\beta$'</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="co"># extent = (1.0, 20.0, 1.0, 20.0)</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="co">#cax = plt.imshow(image, extent=extent)</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>fig.colorbar(cax, label<span class="op">=</span><span class="st">'- log-likelihood'</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>likelihood <span class="op">=</span> gpytorch.likelihoods.GaussianLikelihood()</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ExactGPModel(train_x, train_y, likelihood)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a><span class="co"># mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>training_iter <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Find optimal model hyperparameters</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>likelihood.train()</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>model.likelihood.noise <span class="op">=</span> torch.tensor(<span class="fl">0.1</span>)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>model.mean_module.constant <span class="op">=</span> torch.tensor(<span class="fl">5.750</span>)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>model.covar_module.outputscale <span class="op">=</span> torch.tensor(<span class="fl">5.786</span>)</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.05</span>)  <span class="co"># Includes GaussianLikelihood parameters</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>mll <span class="op">=</span> gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>lscale_1_mle, lscale_2_mle <span class="op">=</span> [], []</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(training_iter):</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(train_x)</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="op">-</span>mll(output, train_y)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>    lscale_1_mle.append(model.covar_module.base_kernel.lengthscale.squeeze()[<span class="dv">0</span>].detach().numpy())</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>    lscale_2_mle.append(model.covar_module.base_kernel.lengthscale.squeeze()[<span class="dv">1</span>].detach().numpy())</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>plt.plot(lscale_1_mle,lscale_2_mle, color<span class="op">=</span><span class="st">'black'</span>, label<span class="op">=</span><span class="st">'Opt. route'</span>)</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>plt.scatter(lscale_1_mle[<span class="dv">0</span>],lscale_2_mle[<span class="dv">0</span>], color<span class="op">=</span><span class="st">'red'</span>, marker<span class="op">=</span><span class="st">'*'</span>, s<span class="op">=</span><span class="dv">200</span>, label<span class="op">=</span><span class="st">'Starting point'</span>)</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>plt.scatter(lscale_1_mle[<span class="op">-</span><span class="dv">1</span>],lscale_2_mle[<span class="op">-</span><span class="dv">1</span>], color<span class="op">=</span><span class="st">'salmon'</span>, marker<span class="op">=</span><span class="st">'*'</span>, s<span class="op">=</span><span class="dv">200</span>, label<span class="op">=</span><span class="ss">f'</span><span class="sc">{</span>training_iter<span class="sc">}</span><span class="ss"> optimization steps'</span>, zorder<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>model.covar_module.base_kernel.lengthscale <span class="op">=</span> torch.tensor([<span class="fl">1.</span>, <span class="fl">1.</span>], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.05</span>)  <span class="co"># Includes GaussianLikelihood parameters</span></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>mll <span class="op">=</span> gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>lscale_1_mle, lscale_2_mle <span class="op">=</span> [], []</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(training_iter):</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(train_x)</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="op">-</span>mll(output, train_y)</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>    lscale_1_mle.append(model.covar_module.base_kernel.lengthscale.squeeze()[<span class="dv">0</span>].detach().numpy())</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>    lscale_2_mle.append(model.covar_module.base_kernel.lengthscale.squeeze()[<span class="dv">1</span>].detach().numpy())</span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>plt.plot(lscale_1_mle,lscale_2_mle, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>plt.scatter(lscale_1_mle[<span class="dv">0</span>],lscale_2_mle[<span class="dv">0</span>], color<span class="op">=</span><span class="st">'red'</span>, marker<span class="op">=</span><span class="st">'*'</span>, s<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>plt.scatter(lscale_1_mle[<span class="op">-</span><span class="dv">1</span>],lscale_2_mle[<span class="op">-</span><span class="dv">1</span>], color<span class="op">=</span><span class="st">'salmon'</span>, marker<span class="op">=</span><span class="st">'*'</span>, s<span class="op">=</span><span class="dv">200</span>, zorder<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'lower left'</span>)</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="bo_example_files/figure-html/cell-6-output-1.png" width="591" height="438" class="figure-img"></p>
<figcaption class="figure-caption">An MLE optimization run using the Adam optimizer for GPR kernel lenghtscales (outputscales, mean function scale, and likelihood noise are fixed). Even for a uni-model landscape, if the mode is very flat, it can result in many optimizers getting stuck. This is why multiple restart optimizers are useful for global hyperparameter finding.</figcaption>
</figure>
</div>
</div>
</div>
<p>Remember, we will use the surrogate model as a proxy for finding the optimal model inputs, therefore, if we just blindly trust the MLE of hyperparameters selection, we may be wrong! In the case that we are wrong, we don’t have much in terms of quantifying how uncertain we are about the wrong fit just fitting the MLE.</p>
<p>Regardless, for small dimensionality and sufficiently uni-modal landscape, MLE gives a decent approximation.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_evaluation(model, likelihood, acq_suggest <span class="op">=</span> <span class="va">None</span>): </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Set into eval mode</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  model.<span class="bu">eval</span>()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  likelihood.<span class="bu">eval</span>()</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Test points</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  n1, n2 <span class="op">=</span> <span class="dv">75</span>, <span class="dv">75</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  alphas, betas <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, n1), np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, n2)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Make predictions</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">with</span> torch.no_grad(), gpytorch.settings.fast_computations(log_prob<span class="op">=</span><span class="va">False</span>, covar_root_decomposition<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>      test_x <span class="op">=</span> torch.from_numpy(np.dstack(np.meshgrid(alphas, betas)).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>      predictions <span class="op">=</span> likelihood(model(test_x))</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>      mean <span class="op">=</span> predictions.mean</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>      x_range_test <span class="op">=</span> torch.tile(torch.from_numpy(x_range), (test_x.shape[<span class="dv">0</span>], <span class="dv">1</span>))</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>      model_output <span class="op">=</span> model_f_torch(x_range_test, test_x[:, <span class="dv">0</span>].unsqueeze(<span class="op">-</span><span class="dv">1</span>), test_x[:, <span class="dv">1</span>].unsqueeze(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>      discrepency_output <span class="op">=</span> disc_fun_torch(experimental_result_torch.repeat((model_output.shape[<span class="dv">0</span>], <span class="dv">1</span>)), model_output).mean(<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> x_range_test.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>  fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>  extent <span class="op">=</span> (alphas.<span class="bu">min</span>(), alphas.<span class="bu">max</span>(), betas.<span class="bu">min</span>(), betas.<span class="bu">max</span>())</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>  ax[<span class="dv">1</span>].imshow(np.flip(mean.detach().numpy().reshape(n1, n2), <span class="dv">0</span>), extent<span class="op">=</span>extent, cmap<span class="op">=</span>plt.get_cmap(<span class="st">'plasma'</span>))</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>  ax[<span class="dv">0</span>].set_title(<span class="st">'True Discrepency'</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>  ax[<span class="dv">0</span>].imshow(np.flip(discrepency_output.detach().numpy().reshape(n1, n2), <span class="dv">0</span>), extent<span class="op">=</span>extent, cmap<span class="op">=</span>plt.get_cmap(<span class="st">'plasma'</span>))</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>  ax[<span class="dv">1</span>].set_title(<span class="st">'GPR values'</span>)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> a <span class="kw">in</span> ax: </span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    a.scatter(train_x[:, <span class="dv">0</span>], train_x[:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'grey'</span>)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    a.set_xlabel(<span class="vs">r'$\alpha$'</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    a.set_ylabel(<span class="vs">r'$\beta$'</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    a.scatter(alpha_target, beta_target, marker<span class="op">=</span><span class="st">'*'</span>, color<span class="op">=</span><span class="st">'green'</span>, s<span class="op">=</span><span class="dv">400</span>)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> acq_suggest <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: </span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>].scatter(<span class="op">*</span>acq_suggest[<span class="dv">0</span>], color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Acquisition Suggestion'</span>, s<span class="op">=</span><span class="dv">400</span>, marker<span class="op">=</span><span class="st">'*'</span>)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>: </span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>].scatter(<span class="op">*</span>test_x[torch.argmin(mean)], color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Current optimum'</span>)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>  plt.show()</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>likelihood_mle <span class="op">=</span> gpytorch.likelihoods.GaussianLikelihood()</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>model_mle <span class="op">=</span> ExactGPModel(train_x, train_y, likelihood)</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="co"># mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>training_iter <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>model_mle.train()</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>likelihood_mle.train()</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model_mle.parameters(), lr<span class="op">=</span><span class="fl">0.05</span>)  <span class="co"># Includes GaussianLikelihood parameters</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>mll <span class="op">=</span> gpytorch.mlls.ExactMarginalLogLikelihood(likelihood_mle, model_mle)</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(training_iter):</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model_mle(train_x)</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="op">-</span>mll(output, train_y)</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Iter </span><span class="sc">%d</span><span class="st">/</span><span class="sc">%d</span><span class="st"> - Loss: </span><span class="sc">%.3f</span><span class="st">   lengthscales: </span><span class="sc">%.3f</span><span class="st">, </span><span class="sc">%.3f</span><span class="st">   noise: </span><span class="sc">%.3f</span><span class="st">  mean length: </span><span class="sc">%.3f</span><span class="st">  outputscale: </span><span class="sc">%.3f</span><span class="st">"</span> <span class="op">%</span> (i <span class="op">+</span> <span class="dv">1</span>, training_iter, loss.item(),</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>    model_mle.covar_module.base_kernel.lengthscale.squeeze()[<span class="dv">0</span>],</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>    model_mle.covar_module.base_kernel.lengthscale.squeeze()[<span class="dv">1</span>],</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>    model_mle.likelihood.noise.item(), </span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>    model_mle.mean_module.constant, </span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>    model_mle.covar_module.outputscale,</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>plot_evaluation(model_mle, likelihood_mle)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Iter 300/300 - Loss: 2.484   lengthscales: 8.825, 3.459   noise: 0.000  mean length: 3.368  outputscale: 4.126</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="bo_example_files/figure-html/cell-7-output-2.png" width="693" height="350" class="figure-img"></p>
<figcaption class="figure-caption">MLE for all hyperparameters of GPR results in decent approximation.</figcaption>
</figure>
</div>
</div>
</div>
<p>Generally, the fit is alright. It’s nice how we already have a close approximation of what should be the optimal value!</p>
</section>
<section id="approach-2-marginalizing-the-hyperparameters-out" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="approach-2-marginalizing-the-hyperparameters-out">Approach 2: Marginalizing the hyperparameters out</h3>
<p>According to Bayesian formalisim <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, we should start with a prior distribution over the hyperparameters, <span class="math inline">\(P(\text{hyper})\)</span>, which is in turn modified using training data <span class="math inline">\(\theta\)</span> to produce a posterior <span class="math inline">\(P(\text{hyper}|\theta)\)</span>. To make predictions, we should then integrate over the posterior. With the above example, the predicted mean output of the GPR is <span class="math inline">\(\hat{\mu}(\theta_i)\)</span> for a given input <span class="math inline">\(\theta_i\)</span> is:</p>
<p><span class="math display">\[\hat{\mu} (\theta_i) = \int \mu_{\text{hyper}} (\theta_i) P(\text{hyper}|\theta) d\text{hyper}\]</span> where <span class="math inline">\(\mu_{\text{hyper}}\)</span> is the preidcted mean for a particular value of <span class="math inline">\(\text{hyper}\)</span>.</p>
<p>In this simple case, this is actually analytically feasable, but with fusion models, typically not. Therefore, we can apply MCMC and its friends. So we perscribe priors distributions over <span class="math inline">\(P(\text{hyper})\)</span> and use MCMC to give us samples from the posterior.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./figures/MCMC_CHAINS_EXAMPLE_crop.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Example MCMC Chains over the GPR hyperparameters</figcaption>
</figure>
</div>
<p>We can then take draws from the above posterior distributions.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>pyro_dict <span class="op">=</span> torch.load(<span class="st">'./pyro_mcmc_out.pth'</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>state_dict <span class="op">=</span> torch.load(<span class="st">'./example_model.pth'</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>likelihood <span class="op">=</span> gpytorch.likelihoods.GaussianLikelihood()</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ExactGPModel(train_x, train_y, likelihood)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>model.load_strict_shapes(<span class="va">False</span>) </span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>model.load_state_dict(state_dict)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>model.pyro_load_from_samples(pyro_dict)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> key, val <span class="kw">in</span> model.named_parameters(): </span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> key <span class="kw">in</span> [<span class="st">'mean_module.raw_constant'</span>]: </span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        num_draws <span class="op">=</span> val.shape[<span class="dv">0</span>]</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>likelihood.<span class="bu">eval</span>()</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>n1, n2 <span class="op">=</span> <span class="dv">75</span>, <span class="dv">75</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>alphas, betas <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, n1), np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, n2)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad(), gpytorch.settings.fast_computations(log_prob<span class="op">=</span><span class="va">False</span>, covar_root_decomposition<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    test_x <span class="op">=</span> torch.from_numpy(np.dstack(np.meshgrid(alphas, betas)).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    expanded_test_x <span class="op">=</span> test_x.unsqueeze(<span class="dv">0</span>).repeat(num_draws, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> likelihood(model(expanded_test_x))</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    mean_out_samples <span class="op">=</span> predictions.mean</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    x_range_test <span class="op">=</span> torch.tile(torch.from_numpy(x_range), (test_x.shape[<span class="dv">0</span>], <span class="dv">1</span>))</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    model_output <span class="op">=</span> model_f_torch(x_range_test, test_x[:, <span class="dv">0</span>].unsqueeze(<span class="op">-</span><span class="dv">1</span>), test_x[:, <span class="dv">1</span>].unsqueeze(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    discrepency_output <span class="op">=</span> disc_fun_torch(experimental_result_torch.repeat((model_output.shape[<span class="dv">0</span>], <span class="dv">1</span>)), model_output).mean(<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> x_range_test.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">15</span>))</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs.ravel()</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>extent <span class="op">=</span> (alphas.<span class="bu">min</span>(), alphas.<span class="bu">max</span>(), betas.<span class="bu">min</span>(), betas.<span class="bu">max</span>())</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'True Discrepency'</span>)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].imshow(np.flip(discrepency_output.detach().numpy().reshape(n1, n2), <span class="dv">0</span>), extent<span class="op">=</span>extent, cmap<span class="op">=</span>plt.get_cmap(<span class="st">'plasma'</span>))</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'GPR Mean Realization'</span>)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].imshow(np.flip(mean_out_samples.mode(<span class="dv">0</span>)[<span class="dv">0</span>].detach().numpy().reshape(n1, n2), <span class="dv">0</span>), extent<span class="op">=</span>extent, cmap<span class="op">=</span>plt.get_cmap(<span class="st">'plasma'</span>))</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(<span class="op">*</span>test_x[torch.argmin(mean_out_samples.mode(<span class="dv">0</span>)[<span class="dv">0</span>])], color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Current optimum'</span>)</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>to_plot <span class="op">=</span> <span class="dv">114</span></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, to_plot <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="dv">100</span>, <span class="dv">8</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">650</span>]):</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">2</span><span class="op">+</span>k].imshow(np.flip(mean_out_samples[to_plot].detach().numpy().reshape(n1, n2), <span class="dv">0</span>), extent<span class="op">=</span>extent, cmap<span class="op">=</span>plt.get_cmap(<span class="st">'plasma'</span>))</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">2</span><span class="op">+</span>k].scatter(<span class="op">*</span>test_x[torch.argmin(mean_out_samples[to_plot])], color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Current optimum'</span>)</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">2</span><span class="op">+</span>k].set_title(<span class="ss">f'GPR Sample </span><span class="sc">{</span>to_plot<span class="sc">}</span><span class="ss"> Realization'</span>)</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a <span class="kw">in</span> ax: </span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>    a.set_xlabel(<span class="vs">r'$\alpha$'</span>)</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>    a.set_ylabel(<span class="vs">r'$\beta$'</span>)</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>    a.scatter(alpha_target, beta_target, marker<span class="op">=</span><span class="st">'*'</span>, color<span class="op">=</span><span class="st">'green'</span>, s<span class="op">=</span><span class="dv">400</span>)</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./figures/MCMC_POSTERIOR_REALIZATION.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Example draws from posterior realization, here the red dot is the analyitical minimum of the posterior draw. Green start is the actual minimum.</figcaption>
</figure>
</div>
<p>Now that we have a fitted surrogate model, we would like a way to query it to obtain new points (<span class="math inline">\(\alpha, \beta\)</span>), that hopefully better fit the experimental data with our model.</p>
</section>
</section>
<section id="aquiring-new-points-from-the-surrogate" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="aquiring-new-points-from-the-surrogate"><span class="header-section-number">2.4</span> Aquiring new points from the surrogate</h2>
<p>This is done using an <em>aquisition function</em>: <span class="math display">\[ \alpha (\theta | \text{hyper}): \mathcal{R}^d \rightarrowtail \mathcal{R}\]</span></p>
<p>Aquisition functions essentially measure the quality of a point <span class="math inline">\(\theta\)</span> (here, once again our <span class="math inline">\(\alpha, \beta\)</span>), and decide at which location of <span class="math inline">\(\theta \in \mathcal{D}\)</span> is most ‘promising’. The acquisition function is based on our surrogte models predictive distribution <span class="math inline">\(p(R | \theta, \text{hyper})\)</span>. Usually, the acquistion function depends on the posterior mean prediction, <span class="math inline">\(\mu(\theta)\)</span>, and the associated posterior uncertainty, <span class="math inline">\(\sigma(\theta)\)</span>.</p>
<p>A popular acquisition function is the Expected improvement<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> <span class="math display">\[\alpha_{\text{EI}} (\theta | \text{hyper}) = \mathcal{E}_{p(R | \theta, \text{hyper})} \left[ \text{min}(R^* - R(\theta), 0 )\right]\]</span></p>
<p>The way we use the acquisition function will change depending on if we took approach 1 or 2 from above.</p>
<section id="approach-1-using-the-mle-surrogate" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="approach-1-using-the-mle-surrogate">Approach 1: Using the MLE surrogate</h3>
<p>From our surrogate model with <span class="math inline">\(\text{hyper}\)</span> determined by MLE, we can plug in <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma\)</span> into the above equation.</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>model_mle.<span class="bu">eval</span>() </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>likelihood_mle.<span class="bu">eval</span>()</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> botorch </span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>acq_fun <span class="op">=</span> botorch.acquisition.analytic.ExpectedImprovement(model_mle, best_f <span class="op">=</span> train_y.<span class="bu">min</span>(), maximize<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>bounds <span class="op">=</span> torch.stack([torch.ones(<span class="dv">2</span>)<span class="op">*-</span><span class="dv">10</span>, torch.ones(<span class="dv">2</span>)<span class="op">*</span><span class="fl">10.0</span>])</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>candidate, acq_val <span class="op">=</span> botorch.optim.optimize_acqf(acq_fun, bounds<span class="op">=</span>bounds, q<span class="op">=</span><span class="dv">1</span>, num_restarts<span class="op">=</span><span class="dv">5</span>, raw_samples<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>plot_evaluation(model_mle, likelihood_mle, candidate)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="bo_example_files/figure-html/cell-9-output-1.png" width="693" height="350"></p>
</div>
</div>
<p>We can see already that the MLE optimized model gives a very good first guess on where to sample from next!</p>
</section>
<section id="approach-2-using-the-integrated-predictive-posterior" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="approach-2-using-the-integrated-predictive-posterior">Approach 2: Using the integrated predictive posterior</h3>
<p>Since we have marginalized out the hyperparameters, our aquisition function becomes <span class="citation" data-cites="snoek2012practical">Ath et al. (<a href="#ref-death2021" role="doc-biblioref">2021</a>)</span>:</p>
<p><span class="math display">\[\text{acq}(x | R, X) = \int \text{acq}(x|\text{hyper}) P(\text{hyper}|R) d\text{hyper}\]</span></p>
<p>but since we have performed MCMC integration, this is discretized:</p>
<p><span class="math display">\[\text{acq}(x | R) \approx \frac{1}{M} \sum_{m=1}^M \text{acq}(x | \text{hyper}^m )\]</span></p>
<p>where <span class="math inline">\({\text{hyper}^1, \dots, \text{hyper}^m}\)</span> are samples drawn from <span class="math inline">\(p(\text{hyper}|R)\)</span>. In essence, we draw models via the hyperparameter posterior, <span class="math inline">\(\theta\)</span>, which yield us <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma\)</span> for each model, and apply the acquisition function to each, averaging over all outputs as our desired point.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./figures/ACQ_OVER_MCMC_INT.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Example acquisition function by averaging over GP integration realizations</figcaption>
</figure>
</div>
</section>
</section>
<section id="the-full-bo-algorithm" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="the-full-bo-algorithm"><span class="header-section-number">2.5</span> The full BO algorithm</h2>
<p>The full BO algorithm looks like the following: <img src="./figures/bo_algorithm.png" class="img-fluid" alt="Bo Algorithm"></p>
<p>Lets show the evolution in practice. We create 50 different sets of 8 ‘initial data’ points (<span class="math inline">\({(\alpha_i, \beta_i), R_i}\)</span>), with <span class="math inline">\(\alpha, \beta\)</span> sampled from uniform distributions and pass them through the model to get <span class="math inline">\(R_i\)</span>. We then perform the BO algorithm above, for 30 acquisition iterations.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./figures/MLE_BO_FULL.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">BO implementation for 50 trials of 30 aquisition iterations with the GPR fitted via MLE. Each iteration samples 1 new point. The median best optimal value insofar is plotted, with the 10-90 percent quantiles as error bars.</figcaption>
</figure>
</div>
<p>From the above plot, we can see that the BO system does not always fully converge! Reasons for this may be that i) our aquisition function is not ideal, ii) the MLE estimation of GP hyperparameters fails, iii) the combination of i) and ii) leads the model to find a local minimum and exploit that.</p>
<p>Regardless, the median of trials do converge to a minimum, which is plotted below.</p>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># optimal value found: tensor(0.0067) tensor([7.8547, 1.4062])</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure() </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>alpha_found, beta_found <span class="op">=</span> [<span class="fl">7.8547</span>, <span class="fl">1.4062</span>]</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># alpha_target, beta_target = 7.9, 1.4</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>plt.scatter(x_range, experimental_result, label<span class="op">=</span><span class="vs">r'Target $\theta$; $\alpha$=</span><span class="sc">{alpha:.2}</span><span class="vs">; $\beta$=</span><span class="sc">{beta:.2}</span><span class="vs">'</span>.<span class="bu">format</span>(alpha<span class="op">=</span>alpha_target, beta<span class="op">=</span>beta_target), )</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>plt.plot(x_range, model_f(x_range, alpha_found, beta_found), label<span class="op">=</span><span class="vs">r'BO Optimum $\theta$; $\alpha$=</span><span class="sc">{alpha:.4}</span><span class="vs">; $\beta$=</span><span class="sc">{beta:.4}</span><span class="vs">'</span>.<span class="bu">format</span>(alpha<span class="op">=</span>alpha_found, beta<span class="op">=</span>beta_found), color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Result of coverged BO trial'</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r'$f(x, \alpha, \beta)$'</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="bo_example_files/figure-html/cell-10-output-1.png" width="610" height="449" class="figure-img"></p>
<figcaption class="figure-caption">The model output for the given (MLE) BO output after 30 iterations.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="obtaining-uncertainties" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="obtaining-uncertainties"><span class="header-section-number">2.6</span> Obtaining uncertainties</h2>
<p>We can recover the optimal <span class="math inline">\(\theta\)</span> determined by the BO algorithm, as well as our uncertainty regarding it. This is our <em>forward</em> uncertainty. To do this, we take the fitted model (either by MLE or margnilzation over hyperparameters), and perform MCMC integration over the posterior, w.r.t input parameters, i.e., we sample <span class="math inline">\(\theta\)</span> around where the model posterior is at its minimum.</p>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># using a fitted model </span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>likelihood, model <span class="op">=</span> mll.likelihood, mll.model </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>likelihood.<span class="bu">eval</span>()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>() </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyro.infer <span class="im">import</span> NUTS, MCMC </span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyro </span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyro.distributions <span class="im">as</span> dist</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> arviz <span class="im">as</span> az </span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mcmc_model(): </span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>   alpha <span class="op">=</span> pyro.sample(<span class="st">'alpha'</span>, dist.Normal(<span class="dv">0</span>, <span class="dv">5</span>))</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>   beta <span class="op">=</span> pyro.sample(<span class="st">'beta'</span>, dist.Normal(<span class="dv">0</span>, <span class="dv">5</span>))</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>   inputs <span class="op">=</span> torch.stack([alpha, beta], <span class="dv">0</span>).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>   model_output <span class="op">=</span> likelihood(model(inputs))</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>   <span class="cf">return</span> pyro.sample(<span class="st">'y'</span>, dist.Normal(model_output.mean, model_output.variance), obs<span class="op">=</span>torch.tensor(<span class="fl">0.0</span>))</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>nuts_kernel <span class="op">=</span> NUTS(mcmc_model)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>mcmc <span class="op">=</span> MCMC(nuts_kernel, num_samples<span class="op">=</span><span class="dv">500</span>, num_chains<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>mcmc.run()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./figures/MCMC_INVERSE_UQ.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">MCMC over posterior yields us the uncertainty of our optimal parameters w.r.t to the BO task.</figcaption>
</figure>
</div>
<p>The mean of the distribution should represent our optimal value(s), the spread our confidence that it is the optimal value(s). Additionally, we can use this distribution to gather <em>inverse</em> uncertainty about how the model is affected due to changes in <span class="math inline">\(\theta\)</span>.</p>
<p>We can use the sample from the distributions determined from the MCMC sampling and and pass those through the model.</p>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>var_alpha, var_beta <span class="op">=</span> <span class="fl">0.2</span>, <span class="fl">0.3</span> </span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>mu_alpha, mu_beta <span class="op">=</span> <span class="fl">7.8547</span>, <span class="fl">1.4062</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>alpha_sample <span class="op">=</span> torch.FloatTensor(<span class="dv">40</span>).normal_(mu_alpha, var_alpha)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>beta_sample <span class="op">=</span> torch.FloatTensor(<span class="dv">40</span>).normal_(mu_beta, var_beta)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> torch.stack([alpha_sample, beta_sample], <span class="dv">1</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure() </span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha, beta <span class="kw">in</span> <span class="bu">zip</span>(alpha_sample, beta_sample): </span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>  plt.plot(x_range, model_f(x_range, alpha.item(), beta.item()), label<span class="op">=</span><span class="vs">r'$\alpha$=</span><span class="sc">{alpha:.4}</span><span class="vs">; $\beta$=</span><span class="sc">{beta:.4}</span><span class="vs">'</span>.<span class="bu">format</span>(alpha<span class="op">=</span>alpha, beta<span class="op">=</span>beta), color<span class="op">=</span><span class="st">'salmon'</span>)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>plt.scatter(x_range, experimental_result, zorder<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Passing the MCMC determined posteriors back to the model for inverse UQ'</span>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r'$f(x, \alpha, \beta)$'</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="bo_example_files/figure-html/cell-12-output-1.png" width="629" height="449" class="figure-img"></p>
<figcaption class="figure-caption">Inverse uncertainty propogated back to the model by using the MCMC determined distributions. One could take the expectation over these distributions, i.e., mean and standard deviation for a single line and shaded region.</figcaption>
</figure>
</div>
</div>
</div>
<p>Great! For a single experimental result, we now know our optimal values, how certain we are about them, and how that uncertainty affects the model output.</p>
</section>
<section id="scaling-to-multiple-experiments" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="scaling-to-multiple-experiments"><span class="header-section-number">2.7</span> Scaling to multiple experiments</h2>
<p>Imagine that we had the following experimental results from two different sets of experiments with similar configurations respectively (e.g., Deuterium seeded vs Tritium seeded):</p>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># example D experiments  </span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># example T experiments </span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>alpha_d <span class="op">=</span> np.random.randn(<span class="dv">10</span>) <span class="op">+</span> <span class="dv">4</span> </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>beta_d <span class="op">=</span> <span class="fl">0.6</span><span class="op">*</span>np.random.randn(<span class="dv">10</span>) <span class="op">-</span> <span class="dv">2</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>theta_d <span class="op">=</span> np.stack([alpha_d, beta_d], <span class="dv">1</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>alpha_t <span class="op">=</span> np.random.randn(<span class="dv">10</span>) <span class="op">-</span> <span class="dv">3</span> </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>beta_t <span class="op">=</span> <span class="fl">1.5</span><span class="op">*</span>np.random.randn(<span class="dv">10</span>) <span class="op">-</span> <span class="dv">4</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>theta_t <span class="op">=</span> np.stack([alpha_t, beta_t], <span class="dv">1</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>tiled_xrange <span class="op">=</span> np.tile(x_range, (<span class="dv">10</span>, <span class="dv">1</span>))</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>experimental_results_t <span class="op">=</span> model_f(tiled_xrange, theta_t[:, <span class="dv">0</span>:<span class="dv">1</span>], theta_t[:, <span class="dv">1</span>:]) <span class="op">+</span> <span class="fl">0.4</span><span class="op">*</span>np.random.randn(<span class="dv">10</span>, <span class="dv">25</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>experimental_results_d <span class="op">=</span> model_f(tiled_xrange, theta_d[:, <span class="dv">0</span>:<span class="dv">1</span>], theta_d[:, <span class="dv">1</span>:]) <span class="op">+</span> <span class="fl">0.4</span><span class="op">*</span>np.random.randn(<span class="dv">10</span>, <span class="dv">25</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>)) </span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>plt.plot(tiled_xrange.T, experimental_results_t.T, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>plt.plot(tiled_xrange.T, experimental_results_d.T, color<span class="op">=</span><span class="st">'salmon'</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="bo_example_files/figure-html/cell-13-output-1.png" width="441" height="411" class="figure-img"></p>
<figcaption class="figure-caption">Two sets of experiments.</figcaption>
</figure>
</div>
</div>
</div>
<p>We would have the option of the following choices of implementation:</p>
<ul>
<li>try to find a single set of <span class="math inline">\(\theta\)</span> that best fit all of a given set of experiments (e.g., <span class="math inline">\(\theta\)</span> for blue lines above)
<ul>
<li>This will modify our discrepency function as being the average discrepncy across the given experiments</li>
</ul></li>
<li>find <span class="math inline">\(\theta\)</span> for each experiment, and consider the distribution of <span class="math inline">\(\theta\)</span> for each set</li>
</ul>
<p><strong>Note</strong>: This assumes we know <em>a priori</em> about the different experiments.</p>
<p>But, I am open to better ideas.</p>
</section>
<section id="references" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="references"><span class="header-section-number">2.8</span> References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-death2021" class="csl-entry" role="listitem">
Ath, George De, Richard M Everson, Jonathan E Fieldsend, and Jonathan E 2021 Fieldsend. 2021. <span>“How Bayesian Should Bayesian Optimisation Be?”</span> <a href="https://doi.org/10.1145/3449726.3463164">https://doi.org/10.1145/3449726.3463164</a>.
</div>
<div id="ref-RasmussenW06" class="csl-entry" role="listitem">
Rasmussen, Carl Edward, and Christopher K. I. Williams. 2006. <em>Gaussian Processes for Machine Learning</em>. Adaptive Computation and Machine Learning. <span>MIT</span> Press. <a href="https://www.worldcat.org/oclc/61285753">https://www.worldcat.org/oclc/61285753</a>.
</div>
<div id="ref-snoek2012practical" class="csl-entry" role="listitem">
Snoek, Jasper, Hugo Larochelle, and Ryan P. Adams. 2012. <span>“Practical Bayesian Optimization of Machine Learning Algorithms.”</span> <a href="https://arxiv.org/abs/1206.2944">https://arxiv.org/abs/1206.2944</a>.
</div>
<div id="ref-Rasmussen1995" class="csl-entry" role="listitem">
Williams, Christopher, and Carl Rasmussen. 1995. <span>“Gaussian Processes for Regression.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by D. Touretzky, M. C. Mozer, and M. Hasselmo. Vol. 8. MIT Press. <a href="https://proceedings.neurips.cc/paper_files/paper/1995/file/7cce53cf90577442771720a370c3c723-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/1995/file/7cce53cf90577442771720a370c3c723-Paper.pdf</a>.
</div>
<div id="ref-Wu2018" class="csl-entry" role="listitem">
Wu, Xu, Tomasz Kozlowski, Hadi Meidani, and Koroush Shirvan. 2018. <span>“Inverse Uncertainty Quantification Using the Modular Bayesian Approach Based on Gaussian Process, Part 1: Theory.”</span> <em>Nuclear Engineering and Design</em> 335: 339–55. https://doi.org/<a href="https://doi.org/10.1016/j.nucengdes.2018.06.004">https://doi.org/10.1016/j.nucengdes.2018.06.004</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>This formulation is more or less copied directly from <em>Gaussian Processes for Regression</em> from Williams and Rasmussen.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>A nice overview is given in <a href="https://ekamperi.github.io/machine%20learning/2021/06/11/acquisition-functions.html">this blog post</a> where <span class="math inline">\(R^*\)</span> is the best function value observed so far, i.e., minimum discrepency. This measures the expected negative improvement (since we are minimizing) over the best function value observed so far.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./bootstrap_current.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">On the bootstrap current in tokamaks</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->



</body></html>