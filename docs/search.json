[
  {
    "objectID": "bo_example.html",
    "href": "bo_example.html",
    "title": "2  Bayesian Optimization Example for Model Validation",
    "section": "",
    "text": "3 Further notes"
  },
  {
    "objectID": "bo_example.html#example-model",
    "href": "bo_example.html#example-model",
    "title": "2  Bayesian Optimization Example for Model Validation",
    "section": "2.1 Example Model",
    "text": "2.1 Example Model\nLet’s consider the model \\[f(x, \\alpha, \\beta) = \\alpha\\sin(x) + \\beta e^{-x}\\]\nThe free parameters are \\(\\alpha, \\beta\\).\n\n\nCode\nimport matplotlib.pyplot as plt \nimport numpy as np \nimport matplotlib as mpl \n\nmodel_f = lambda x, alpha, beta: (alpha*np.sin(x) + beta*np.exp(-x)) #  / gamma*np.cos(x)\n\nnum_samples = 25\nx_range = np.linspace(-1, 1, num_samples)\n\nfig = plt.figure()\nfor i in range(6): \n  alpha = 4.0 * np.random.randn()\n  beta = 1.0 * np.random.randn() + 2\n  gamma = 1 * np.random.randn()\n  plt.plot(x_range, model_f(x_range, alpha, beta), label=r'$\\alpha$={alpha:.2}; $\\beta$={beta:.2}'.format(alpha=alpha, beta=beta))\nplt.legend()\nplt.title(r'Example Model output for various $\\theta$')\nplt.ylabel(r'$f(x, \\alpha, \\beta)$')\nplt.xlabel('x')\nplt.ylim(-10, 10)\nplt.show()\n\n\n\n\n\nLet’s say we have some noisy measurements now that we want to validate the model against:\n\n\nCode\nx_range = np.linspace(-1.0, 1.0, num_samples)\nalpha_target, beta_target = 7.9, 1.4\nexperimental_result = model_f(x_range, alpha_target, beta_target) + 0.4*np.random.randn(x_range.shape[0])\ntarget_model = model_f(x_range, alpha_target, beta_target)\nfig = plt.figure() \nplt.plot(x_range, target_model, label=r'Target $y$; $\\alpha$={alpha:.2}; $\\beta$={beta:.2}'.format(alpha=alpha_target, beta=beta_target), color='black', alpha=0.3)\nplt.scatter(x_range, experimental_result, label='Experiment: $\\hat{y}$',)\nplt.legend()\nplt.xlabel('x')\nplt.ylim(-10, 10)\nplt.show()\n\n\n\n\n\nThe goal is then to find \\(\\alpha = 7.9\\) and \\(\\beta = 1.4\\). Additionally, it would be nice to know how certain we are about \\(\\alpha, \\beta\\)."
  },
  {
    "objectID": "bo_example.html#framing-validation-as-an-optimization-problem",
    "href": "bo_example.html#framing-validation-as-an-optimization-problem",
    "title": "2  Bayesian Optimization Example for Model Validation",
    "section": "2.2 Framing validation as an optimization problem",
    "text": "2.2 Framing validation as an optimization problem\nFormally, we want to find \\[\\underset{{\\theta \\in \\mathcal{D}}}{\\text{argmin}} ( r(f(x, \\theta), \\hat{y}))\\] where \\(r(f, \\hat{y})\\) is a mapping \\(r: \\mathbb{R}^d\\times\\mathbb{R}^d \\rightarrow \\mathbb{R}^+\\) describing the discrepency between the model output and experimental observation, and \\(\\mathcal{D}\\) is the space in which parameters \\(\\theta\\) are constrained.\nFor our example, we can use the L-2 norm as a discrepency function: \\[r(f(x, \\theta), \\hat{y}) = | f(x, \\theta) - \\hat{y} | ^2_2\\]\nTo visualize the discrepency function, we can sample \\(\\alpha, \\beta\\) and compute \\(r\\).\n\n\nCode\nfrom matplotlib import colors as col # .colors import Normalize\n\nalphas = np.linspace(-10, 10, 5)\nbetas = np.linspace(-10, 10, 5)\n\ndisc_func = lambda y, yhat: ((y - yhat)**2).mean(-1) / yhat.shape[0]# np.linalg.norm# lambda y, yhat: (y - yhat)**2\n\ngrid = np.dstack(np.meshgrid(alphas, betas)).reshape(-1, 2)\n\ndiscs = np.empty(grid.shape[0])\nfor i, pair in enumerate(grid): \n  model_out = model_f(x_range, *pair)\n  discrepency = disc_func(model_out, experimental_result)\n  discs[i] = discrepency\n\ncm = plt.get_cmap('plasma')\nscmap = plt.cm.ScalarMappable(col.Normalize(vmin=0.0, vmax=max(discs)), cm)\ncolors = scmap.to_rgba(discs)\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\nfor c, pair in zip(colors, grid): \n  axs[0].plot(x_range, model_f(x_range, *pair), c=c)\naxs[0].scatter(x_range, experimental_result, label='Experiment: $\\hat{y}$', color='black')\naxs[1].scatter(grid[:, 0], grid[:, 1], c=discs, vmin=0, vmax=max(discs), cmap=cm)\naxs[1].scatter(alpha_target, beta_target, marker='*', s=50, color=scmap.to_rgba(disc_func(target_model, experimental_result)))\naxs[1].set_xlabel(r'$\\alpha$')\naxs[1].set_ylabel(r'$\\beta$')\naxs[0].set_title('Model outputs')\naxs[1].set_title(r'Discrepency Function against various $\\theta$')\nfig.colorbar(scmap, ax = axs[1], label='r')\nplt.show()\n\n\n\n\n\nIn real world examples, we do not have direct (analytical) access to \\(r\\). Therefore, we want to make an approximate of \\(r\\)."
  },
  {
    "objectID": "bo_example.html#surrogate-model-of-r",
    "href": "bo_example.html#surrogate-model-of-r",
    "title": "2  Bayesian Optimization Example for Model Validation",
    "section": "2.3 Surrogate model of \\(r\\)",
    "text": "2.3 Surrogate model of \\(r\\)\n\\(r\\) can be approximated by any number of functions. Therefore, a Gaussian Process Regression (GPR) is a good start. A good overview of GPRs can be found in Rasmussen and Williams, GPML 2006. The points relevant to this discusison are that: i) GPR defines a family (distribution) of functions, normally Guassian, and ii) like any model, a GPR has hyperparameters, typically \\(D + 3\\) parameters, for \\(D\\) dimensionality of the data you want to fit (in this case 2). The traditional GPR thus learns a model that outputs a distribution: \\[\\hat{r} \\sim \\mathcal{N}(\\mu (\\theta), \\sigma (\\theta))\\]\nwhere \\(\\mu\\) and \\(\\sigma\\) are deterimed by what are called the kernel and mean function of the GPR. The kernel and mean function have hyperparameters that we must find. Another key point is that the kernel and mean function are differentiable functions.\nWe are only as good as our surrogate model. To find a good surrogate model, we must find the proper hyperparameters of the kernel and mean function of the GPR.\n\n2.3.1 Approach 1: Point-wise estimation via maximizng the log-likelihood\nIn this example, we will use a Gaussian likelihood GPR, i.e., we say that the probability density of observing a point \\(R = r(\\theta)\\) that is generated by a Gaussian distribution is given by:\n\\[P(R, \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\left( -\\frac{(R - \\mu)^2}{2\\sigma^2}\\right)\\] where, one again, \\(\\mu\\) and \\(\\sigma\\) are outputs of our GPR. Notice this probability distribution has a maximum if the mean output of our GPR matches that of the point we observe, i.e., \\(\\mu = R\\). If we have mutliple points to fit, \\(\\vec{R} = (r(\\theta_1), r(\\theta_2), r(\\theta_3), \\dots, R_i)\\), then the total joint probability distribution of observing all the points is given by the product of their likelihood:\n\\[P(\\vec{R}, \\mu, \\sigma) = \\prod_i \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp \\left( -\\frac{(R_i - \\mu)^2}{2\\sigma^2}\\right)\\]\nHowever, it is easy to see that with many points, we will likely hit some numerical underflow, therefore we can make use of the logarithm:, \\[\\ln(P(\\vec{R}, \\mu, \\sigma)) = i\\ln \\left(\\frac{1}{\\sigma \\sqrt{2\\pi}}\\right) - \\sum_i \\left(\\frac{(R_i - \\mu)^2}{2\\sigma^2}\\right)\\]\nWe can then differentiate this function in order to find the maximum and apply our favourite gradient based optimizer to find the hyperparameters of the kernel and mean function that determine \\(\\mu\\) and \\(\\sigma\\). This approach is called Maximum Likelihood Estimation (MLE). Note: This is called point-wise because we are estimating the hyperparameters of the GP ‘per point’ we use to fit the GP.\nBelow is an example of the ‘lengthscale’ parameters of the covariance function, where the countours are the(negative) log-likelihood estimation.\n\n\nCode\nimport gpytorch \nimport torch \nimport botorch \n\nmodel_f_torch = lambda x, alpha, beta: (alpha*torch.sin(x) + beta*torch.exp(-x)) \nexperimental_result_torch = torch.from_numpy(experimental_result)\ndisc_fun_torch = torch.nn.MSELoss(reduction='none')\n\ntrain_x = torch.from_numpy(grid).reshape(-1, 2)\nx_range_train = torch.tile(torch.from_numpy(x_range), (train_x.shape[0], 1))\nmodel_output = model_f_torch(x_range_train, train_x[:, 0].unsqueeze(-1), train_x[:, 1].unsqueeze(-1))\ntrain_y = disc_fun_torch(experimental_result_torch.repeat((model_output.shape[0], 1)), model_output).mean(-1) / x_range_train.shape[-1]\n\n\n# We will use the simplest form of GP model, exact inference\nclass ExactGPModel(gpytorch.models.ExactGP, botorch.models.gpytorch.GPyTorchModel):\n    num_outputs = 1\n    def __init__(self, train_x, train_y, likelihood):\n        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean(constant_prior=gpytorch.priors.NormalPrior(0, 1))\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=train_x.shape[-1]))\n    \n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n# initialize likelihood and model\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\nmodel = ExactGPModel(train_x, train_y, likelihood)\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n\n\n\nCode\nmodel.likelihood.noise = torch.tensor(0.1)\nmodel.mean_module.constant = torch.tensor(5.750)\nmodel.covar_module.outputscale = torch.tensor(5.786)\n\nmodel.covar_module.base_kernel.lengthscale = torch.tensor([5.3, 3.535])\n\n\nlscale_1_range = torch.linspace(0.1, 20, 100)\nlscale_2_range = torch.linspace(0.1, 10, 100)\nimage = np.empty((100, 100))\nfor i, lscale_1 in enumerate(lscale_1_range): \n  for j, lscale_2 in enumerate(lscale_2_range): \n    model.covar_module.base_kernel.lengthscale = torch.tensor([lscale_1,lscale_2])\n    # model.likelihood.noise = torch.tensor(noiscale.item())\n    output = model(train_x)\n    loss = -mll(output, train_y)\n    image[i, j] = loss\n\nfig = plt.figure() \ncax = plt.contour(lscale_1_range, lscale_2_range, image, 50)\nplt.yscale('log')\nplt.xscale('log')\nplt.xlabel(r'Length scale $\\alpha$')\nplt.ylabel(r'Length scale $\\beta$')\n# extent = (1.0, 20.0, 1.0, 20.0)\n#cax = plt.imshow(image, extent=extent)\nfig.colorbar(cax, label='- log-likelihood')\n\n\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\nmodel = ExactGPModel(train_x, train_y, likelihood)\n# mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\ntraining_iter = 200\n# Find optimal model hyperparameters\nmodel.train()\nlikelihood.train()\n\n\nmodel.likelihood.noise = torch.tensor(0.1)\nmodel.mean_module.constant = torch.tensor(5.750)\nmodel.covar_module.outputscale = torch.tensor(5.786)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)  # Includes GaussianLikelihood parameters\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\nlscale_1_mle, lscale_2_mle = [], []\nfor i in range(training_iter):\n    optimizer.zero_grad()\n    output = model(train_x)\n    loss = -mll(output, train_y)\n    loss.backward()\n    optimizer.step()\n    lscale_1_mle.append(model.covar_module.base_kernel.lengthscale.squeeze()[0].detach().numpy())\n    lscale_2_mle.append(model.covar_module.base_kernel.lengthscale.squeeze()[1].detach().numpy())\n\nplt.plot(lscale_1_mle,lscale_2_mle, color='black', label='Opt. route')\nplt.scatter(lscale_1_mle[0],lscale_2_mle[0], color='red', marker='*', s=200, label='Starting point')\nplt.scatter(lscale_1_mle[-1],lscale_2_mle[-1], color='salmon', marker='*', s=200, label=f'{training_iter} optimization steps', zorder=30)\n\nmodel.covar_module.base_kernel.lengthscale = torch.tensor([1., 1.], requires_grad=True)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)  # Includes GaussianLikelihood parameters\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\nlscale_1_mle, lscale_2_mle = [], []\nfor i in range(training_iter):\n    optimizer.zero_grad()\n    output = model(train_x)\n    loss = -mll(output, train_y)\n    loss.backward()\n    optimizer.step()\n    lscale_1_mle.append(model.covar_module.base_kernel.lengthscale.squeeze()[0].detach().numpy())\n    lscale_2_mle.append(model.covar_module.base_kernel.lengthscale.squeeze()[1].detach().numpy())\n\n\nplt.plot(lscale_1_mle,lscale_2_mle, color='black')\nplt.scatter(lscale_1_mle[0],lscale_2_mle[0], color='red', marker='*', s=200)\nplt.scatter(lscale_1_mle[-1],lscale_2_mle[-1], color='salmon', marker='*', s=200, zorder=30)\n\nplt.legend(loc='lower left')\nplt.show()\n\n\n\n\n\nAn MLE optimization run using the Adam optimizer for GPR kernel lenghtscales (outputscales, mean function scale, and likelihood noise are fixed). Even for a uni-model landscape, if the mode is very flat, it can result in many optimizers getting stuck. This is why multiple restart optimizers are useful for global hyperparameter finding.\n\n\n\n\nRemember, we will use the surrogate model as a proxy for finding the optimal model inputs, therefore, if we just blindly trust the MLE of hyperparameters selection, we may be wrong! In the case that we are wrong, we don’t have much in terms of quantifying how uncertain we are about the wrong fit just fitting the MLE.\nRegardless, for small dimensionality and sufficiently uni-modal landscape, MLE gives a decent approximation.\n\n\nCode\ndef plot_evaluation(model, likelihood, acq_suggest = None): \n  # Set into eval mode\n  model.eval()\n  likelihood.eval()\n\n  # Test points\n  n1, n2 = 75, 75\n  alphas, betas = np.linspace(-10, 10, n1), np.linspace(-10, 10, n2)\n\n  # Make predictions\n  with torch.no_grad(), gpytorch.settings.fast_computations(log_prob=False, covar_root_decomposition=False):\n      test_x = torch.from_numpy(np.dstack(np.meshgrid(alphas, betas)).reshape(-1, 2))\n      predictions = likelihood(model(test_x))\n      mean = predictions.mean\n      x_range_test = torch.tile(torch.from_numpy(x_range), (test_x.shape[0], 1))\n      model_output = model_f_torch(x_range_test, test_x[:, 0].unsqueeze(-1), test_x[:, 1].unsqueeze(-1))\n      discrepency_output = disc_fun_torch(experimental_result_torch.repeat((model_output.shape[0], 1)), model_output).mean(-1) / x_range_test.shape[-1]\n\n  fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n  extent = (alphas.min(), alphas.max(), betas.min(), betas.max())\n  ax[1].imshow(np.flip(mean.detach().numpy().reshape(n1, n2), 0), extent=extent, cmap=plt.get_cmap('plasma'))\n  \n  \n  ax[0].set_title('True Discrepency')\n  ax[0].imshow(np.flip(discrepency_output.detach().numpy().reshape(n1, n2), 0), extent=extent, cmap=plt.get_cmap('plasma'))\n  ax[1].set_title('GPR values')\n  for a in ax: \n    a.scatter(train_x[:, 0], train_x[:, 1], color='grey')\n    a.set_xlabel(r'$\\alpha$')\n    a.set_ylabel(r'$\\beta$')\n    a.scatter(alpha_target, beta_target, marker='*', color='green', s=400)\n  if acq_suggest is not None: \n    ax[1].scatter(*acq_suggest[0], color='red', label='Acquisition Suggestion', s=400, marker='*')\n  else: \n    ax[1].scatter(*test_x[torch.argmin(mean)], color='red', label='Current optimum')\n  plt.show()\n\nlikelihood_mle = gpytorch.likelihoods.GaussianLikelihood()\nmodel_mle = ExactGPModel(train_x, train_y, likelihood)\n# mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\ntraining_iter = 300\nmodel_mle.train()\nlikelihood_mle.train()\n\noptimizer = torch.optim.Adam(model_mle.parameters(), lr=0.05)  # Includes GaussianLikelihood parameters\n\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood_mle, model_mle)\n\nfor i in range(training_iter):\n    optimizer.zero_grad()\n    output = model_mle(train_x)\n    loss = -mll(output, train_y)\n    loss.backward()\n    optimizer.step()\nprint(\"Iter %d/%d - Loss: %.3f   lengthscales: %.3f, %.3f   noise: %.3f  mean length: %.3f  outputscale: %.3f\" % (i + 1, training_iter, loss.item(),\n    model_mle.covar_module.base_kernel.lengthscale.squeeze()[0],\n    model_mle.covar_module.base_kernel.lengthscale.squeeze()[1],\n    model_mle.likelihood.noise.item(), \n    model_mle.mean_module.constant, \n    model_mle.covar_module.outputscale,\n))\n\nplot_evaluation(model_mle, likelihood_mle)\n\n\nIter 300/300 - Loss: 2.481   lengthscales: 8.830, 3.453   noise: 0.000  mean length: 3.365  outputscale: 4.124\n\n\n\n\n\nMLE for all hyperparameters of GPR results in decent approximation.\n\n\n\n\nGenerally, the fit is alright. It’s nice how we already have a close approximation of what should be the optimal value!\n\n\n2.3.2 Approach 2: Marginalizing the hyperparameters out\nAccording to Bayesian formalisim 1, we should start with a prior distribution over the hyperparameters, \\(P(\\text{hyper})\\), which is in turn modified using training data \\(\\theta\\) to produce a posterior \\(P(\\text{hyper}|\\theta)\\). To make predictions, we should then integrate over the posterior. With the above example, the predicted mean output of the GPR is \\(\\hat{\\mu}(\\theta_i)\\) for a given input \\(\\theta_i\\) is:\n\\[\\hat{\\mu} (\\theta_i) = \\int \\mu_{\\text{hyper}} (\\theta_i) P(\\text{hyper}|\\theta) d\\text{hyper}\\] where \\(\\mu_{\\text{hyper}}\\) is the preidcted mean for a particular value of \\(\\text{hyper}\\).\nIn this simple case, this is actually analytically feasable, but with fusion models, typically not. Therefore, we can apply MCMC and its friends. So we perscribe priors distributions over \\(P(\\text{hyper})\\) and use MCMC to give us samples from the posterior.\nMore details on this method are found at the bottom of the page, but for now I just show the results of MCMC integration with NUTS.\n\n\n\nExample MCMC Chains from integratin\n\n\nWe can then take draws from the above posterior distributions.\n\n\nCode\npyro_dict = torch.load('./pyro_mcmc_out.pth')\nstate_dict = torch.load('./example_model.pth')\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\nmodel = ExactGPModel(train_x, train_y, likelihood)\nmodel.load_strict_shapes(False) \nmodel.load_state_dict(state_dict)\nmodel.pyro_load_from_samples(pyro_dict)\n\n\nfor key, val in model.named_parameters(): \n    if key in ['mean_module.raw_constant']: \n        num_draws = val.shape[0]\n        \nmodel.eval()\nlikelihood.eval()\n\nn1, n2 = 75, 75\nalphas, betas = np.linspace(-10, 10, n1), np.linspace(-10, 10, n2)\n\n# Make predictions\nwith torch.no_grad(), gpytorch.settings.fast_computations(log_prob=False, covar_root_decomposition=False):\n    test_x = torch.from_numpy(np.dstack(np.meshgrid(alphas, betas)).reshape(-1, 2))\n    expanded_test_x = test_x.unsqueeze(0).repeat(num_draws, 1, 1)\n    \n    predictions = likelihood(model(expanded_test_x))\n    mean_out_samples = predictions.mean\n    x_range_test = torch.tile(torch.from_numpy(x_range), (test_x.shape[0], 1))\n    model_output = model_f_torch(x_range_test, test_x[:, 0].unsqueeze(-1), test_x[:, 1].unsqueeze(-1))\n    discrepency_output = disc_fun_torch(experimental_result_torch.repeat((model_output.shape[0], 1)), model_output).mean(-1) / x_range_test.shape[-1]\n\nfig, axs = plt.subplots(3, 2, figsize=(8, 15))\nax = axs.ravel()\nextent = (alphas.min(), alphas.max(), betas.min(), betas.max())\n\nax[0].set_title('True Discrepency')\nax[0].imshow(np.flip(discrepency_output.detach().numpy().reshape(n1, n2), 0), extent=extent, cmap=plt.get_cmap('plasma'))\nax[1].set_title('GPR Mean Realization')\nax[1].imshow(np.flip(mean_out_samples.mode(0)[0].detach().numpy().reshape(n1, n2), 0), extent=extent, cmap=plt.get_cmap('plasma'))\nax[1].scatter(*test_x[torch.argmin(mean_out_samples.mode(0)[0])], color='red', label='Current optimum')\n\nto_plot = 114\nfor k, to_plot in enumerate([100, 8, -1, 650]):\n    ax[2+k].imshow(np.flip(mean_out_samples[to_plot].detach().numpy().reshape(n1, n2), 0), extent=extent, cmap=plt.get_cmap('plasma'))\n    ax[2+k].scatter(*test_x[torch.argmin(mean_out_samples[to_plot])], color='red', label='Current optimum')\n    ax[2+k].set_title(f'GPR Sample {to_plot} Realization')\n    \nfor a in ax: \n    a.set_xlabel(r'$\\alpha$')\n    a.set_ylabel(r'$\\beta$')\n    a.scatter(alpha_target, beta_target, marker='*', color='green', s=400)\nplt.show()\n\n\n\n\n\nExample draws from posterior realization\n\n\nNow that we have a fitted surrogate model, we would like a way to query it to obtain new points (\\(\\alpha, \\beta\\)), that hopefully better fit the experimental data with our model."
  },
  {
    "objectID": "bo_example.html#aquiring-new-points-from-the-surrogate-model",
    "href": "bo_example.html#aquiring-new-points-from-the-surrogate-model",
    "title": "2  Bayesian Optimization Example for Model Validation",
    "section": "2.4 Aquiring new points from the surrogate model",
    "text": "2.4 Aquiring new points from the surrogate model\nThis is done using an aquisition function: \\[ \\alpha (\\theta | \\text{hyper}): \\mathcal{R}^d \\rightarrowtail \\mathcal{R}\\]\nThey essentially measure the quality of a point \\(\\theta\\) (here once again our \\(\\alpha, \\beta\\)), and decide at which location of \\(\\theta \\in \\mathcal{D}\\) is most ‘promising’. The acquisition function is based on our surrogte models predictive distribution \\(p(R | \\theta, \\text{hyper})\\). Usually, the acquistion function depends on the posterior mean prediction, \\(\\mu(\\theta)\\), and the associated posterior uncertainty, \\(\\sigma(\\theta)\\).\nA popular acquisition function is the Expected improvement [Source]: \\[\\alpha_{\\text{EI}} (\\theta | \\text{hyper}) = \\mathcal{E}_{p(R | \\theta, \\text{hyper})} \\left[ \\text{min}(R^* - R(\\theta), 0 )\\right]\\]\nwhere \\(R^*\\) is the best function value observed so far, i.e., minimum discrepency. This measures the expected negative improvement (since we are minimizing) over the best function value observed so far.\nThe way we use the acquisition function will change depending on if we took approach 1 or 2 from above.\n\n2.4.1 Approach 1: Using the MLE surrogate\nFrom our surrogate model with \\(\\text{hyper}\\) determined by MLE, we can plug in \\(\\mu\\), \\(\\sigma\\) into the above equation.\n\n\nCode\nmodel_mle.eval() \nlikelihood_mle.eval()\n\nimport botorch \n\n\nacq_fun = botorch.acquisition.analytic.ExpectedImprovement(model_mle, best_f = train_y.min(), maximize=False)\n\nbounds = torch.stack([torch.ones(2)*-10, torch.ones(2)*10.0])\ncandidate, acq_val = botorch.optim.optimize_acqf(acq_fun, bounds=bounds, q=1, num_restarts=5, raw_samples=20)\nplot_evaluation(model_mle, likelihood_mle, candidate)\n\n\n\n\n\nWe can see already that the MLE optimized model gives a very good first guess on where to sample from next!\n\n\n2.4.2 Approach 2: Using the integrated predictive posterior\nSince we have marginalized out the hyperparameters, our aquisition function becomes:\n\\[\\text{acq}(x | R, X) = \\int \\text{acq}(x|\\text{hyper}) P(\\text{hyper}|R) d\\text{hyper}\\]\nbut since we have performed MCMC integration, this is discretized:\n\\[\\text{acq}(x | R) \\approx \\frac{1}{M} \\sum_{m=1}^M \\text{acq}(x | \\text{hyper}^m )\\]\nwhere \\({\\text{hyper}^1, \\dots, \\text{hyper}^m}\\) are samples drawn from \\(p(\\text{hyper}|R)\\). In essence, we draw models via the hyperparameter posterior, \\(\\theta\\), which yield us \\(\\mu\\), \\(\\sigma\\) for each model, and apply the acquisition function to each, averaging over all outputs as our desired point.\n\n\n\nExample acquisition function by averaging over GP integration realizations"
  },
  {
    "objectID": "bo_example.html#putting-it-all-together",
    "href": "bo_example.html#putting-it-all-together",
    "title": "2  Bayesian Optimization Example for Model Validation",
    "section": "2.5 Putting it all together",
    "text": "2.5 Putting it all together\nThe full BO algorithm looks like the following: \nLets show the evolution in practice."
  },
  {
    "objectID": "bo_example.html#acquisition-function",
    "href": "bo_example.html#acquisition-function",
    "title": "2  Bayesian Optimization Example for Model Validation",
    "section": "3.1 Acquisition function",
    "text": "3.1 Acquisition function\nTBD not finished yet.\n\n3.1.1 BACKUP: notes on GPRs\nGPR defines a family of functions \\[ f(\\boldsymbol{x}) \\sim \\mathcal{GP}(m(\\boldsymbol{x}), k(\\boldsymbol{x},\\boldsymbol{x'}))\\]\nwhere \\(m(\\boldsymbol {x}) = \\mathbb {E}[f(\\boldsymbol {x})]\\) is a mean function, a \\(k(\\boldsymbol {x},\\boldsymbol {x'}) = \\mathbb {E}[(f(\\boldsymbol {x}) - m(\\boldsymbol {x}))(f(\\boldsymbol {x'}) - m(\\boldsymbol {x}'))]\\) is a covariance function, or kernel.\nThe mean function suggests how the expectation of the output,\\(f\\), will change as \\(\\boldsymbol {x}\\) changes. If the mean function is \\(0\\), then as we change \\(\\boldsymbol {x}\\), variance of the mean of \\(f\\) will also not change.\nThe kernel describes the point-to-point variance of \\(f\\), in other words, the smoothness assumption on the possible functions of \\(f\\). A common kernel function is the RBF: \\[k_\\text{RBF}(x_i, x_j) = \\sigma_f^2 \\exp\\left(-\\sum_{i=1}^{d}\\frac{\\left(x_{i,k} - x_{j,k}\\right)^2}{2l_k^2}\\right)\\] where \\(l\\) and \\(\\sigma\\) are the free parameters of the kernel model. These parameters need to be fit.\nAn example is given below of a GPR with RBF kernel and Constant Mean, fit on the grid of points given before. I modify the output and length scale to show how the function changes. Then, I plot mean output of the GPR, i.e., the Expectation over the family of functions output.\n\n\nCode\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\nmodel = ExactGPModel(train_x, train_y, likelihood)\n# mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\ntraining_iter = 100\n# Find optimal model hyperparameters\nmodel.train()\nlikelihood.train()\n\n# Use the adam optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)  # Includes GaussianLikelihood parameters\n\n# \"Loss\" for GPs - the marginal log likelihood\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\nfor i in range(training_iter):\n    optimizer.zero_grad()\n    output = model(train_x)\n    loss = -mll(output, train_y)\n    loss.backward()\n    optimizer.step()\nprint(\"Iter %d/%d - Loss: %.3f   lengthscales: %.3f, %.3f   noise: %.3f  mean length: %.3f  outputscale: %.3f\" % (i + 1, training_iter, loss.item(),\n    model.covar_module.base_kernel.lengthscale.squeeze()[0],\n    model.covar_module.base_kernel.lengthscale.squeeze()[1],\n    model.likelihood.noise.item(), \n    model.mean_module.constant, \n    model.covar_module.outputscale,\n))\n\nplot_evaluation(model, likelihood)\n\n\n\n\nCode\n# Set into eval mode\nmodel.eval()\nlikelihood.eval()\n\n# Initialize plots\n\n# Test points\nn1, n2 = 75, 75\nalphas, betas = np.linspace(-10, 10, n1), np.linspace(-10, 10, n2)\n\n# Make predictions\nwith torch.no_grad(), gpytorch.settings.fast_computations(log_prob=False, covar_root_decomposition=False):\n    # test_x = torch.stack([xv.reshape(n1*n2, 1), yv.reshape(n1*n2, 1)], -1).squeeze(1)\n    test_x = torch.from_numpy(np.dstack(np.meshgrid(alphas, betas)).reshape(-1, 2))\n    predictions = likelihood(model(test_x))\n    mean = predictions.mean\n    x_range_test = torch.tile(torch.from_numpy(x_range), (test_x.shape[0], 1))\n    model_output = model_f_torch(x_range_test, test_x[:, 0].unsqueeze(-1), test_x[:, 1].unsqueeze(-1))\n    discrepency_output = disc_fun_torch(experimental_result_torch.repeat((model_output.shape[0], 1)), model_output).mean(-1) / x_range_test.shape[-1]\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 10))\n# ax[1].scatter(test_x[:, 0], test_x[:, 1], c=mean.detach().numpy(), vmin=0, vmax=max(mean), cmap=cm)\n# print(test_x[torch.argmin(discrepency_output)], min(discrepency_output), test_x[torch.argmin(mean)])\nextent = (alphas.min(), alphas.max(), betas.min(), betas.max())\nax[1].imshow(np.flip(mean.detach().numpy().reshape(n1, n2), 0), extent=extent, cmap=plt.get_cmap('plasma'))\nax[0].scatter(*test_x[torch.argmin(discrepency_output)], color='black')\nax[1].scatter(*test_x[torch.argmin(mean)], color='red', label='Current optimum')\nax[0].set_title('True Discrepency')\nax[0].imshow(np.flip(discrepency_output.detach().numpy().reshape(n1, n2), 0), extent=extent, cmap=plt.get_cmap('plasma'))\naxs[1].scatter(train_x[:, 0], train_x[:, 1], color='grey', s=40)\n# cax = ax[1].imshow(mean.detach().numpy().reshape(n1, n2), extent=extent, cmap=plt.get_cmap('plasma'))\nax[1].set_title('GPR values')\n# fig.colorbar(cax, ax=ax[1])\nplt.show()\n\n\n\n3.1.1.0.1 Relevant papers\n\nhttps://arxiv.org/pdf/1206.2944.pdf\nhttps://proceedings.neurips.cc/paper_files/paper/1995/file/7cce53cf90577442771720a370c3c723-Paper.pdf\nhttps://doi.org/10.1016/j.nucengdes.2018.06.004\nhttps://doi.org/10.1017/S0022377822001210\nhttps://arxiv.org/pdf/2105.00894v1.pdf\n\ncode\n\nRasmussen\nMarganlizing with sequential MC\nhttps://proceedings.neurips.cc/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf\n\nArxiv: https://arxiv.org/pdf/1206.2944.pdf ##### Relevant code links\n\nHyperparameters in gpytorch\nFully bayesian sampling\nRobust Fully Bayesian"
  },
  {
    "objectID": "bo_example.html#footnotes",
    "href": "bo_example.html#footnotes",
    "title": "2  Bayesian Optimization Example for Model Validation",
    "section": "",
    "text": "This formulation is more or less copied directly from Gaussian Processes for Regression from Williams and Rasmussen.↩︎\nThe gpytorch example uses pyro, but I found this to be a nightmare and didn’t work.↩︎"
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Musings",
    "section": "Preface",
    "text": "Preface\nThis is my lab notebook."
  }
]